
#################################
dcpg_train.py ./data/c1_000000-032768.h5 ./data/c1_032768-065536.h5 ./data/c1_065536-098304.h5 ./data/c1_098304-131072.h5 ./data/c1_131072-163840.h5 ./data/c1_163840-170735.h5 ./data/c2_000000-032768.h5 ./data/c2_032768-065536.h5 ./data/c2_065536-098304.h5 ./data/c2_098304-131072.h5 ./data/c2_131072-163840.h5 ./data/c2_163840-174991.h5 ./data/c3_000000-032768.h5 ./data/c3_032768-065536.h5 ./data/c3_065536-098304.h5 ./data/c3_098304-131072.h5 ./data/c3_131072-137553.h5 ./data/c4_000000-032768.h5 ./data/c4_032768-065536.h5 ./data/c4_065536-098304.h5 ./data/c4_098304-131072.h5 ./data/c4_131072-153643.h5 ./data/c5_000000-032768.h5 ./data/c5_032768-065536.h5 ./data/c5_065536-098304.h5 ./data/c5_098304-131072.h5 ./data/c5_131072-155221.h5 ./data/c6_000000-032768.h5 ./data/c6_032768-065536.h5 ./data/c6_065536-098304.h5 ./data/c6_098304-131072.h5 ./data/c6_131072-134244.h5 ./data/c7_000000-032768.h5 ./data/c7_032768-065536.h5 ./data/c7_065536-098304.h5 ./data/c7_098304-131072.h5 ./data/c7_131072-140587.h5 ./data/c8_000000-032768.h5 ./data/c8_032768-065536.h5 ./data/c8_065536-098304.h5 ./data/c8_098304-130549.h5 ./data/c9_000000-032768.h5 ./data/c9_032768-065536.h5 ./data/c9_065536-098304.h5 ./data/c9_098304-124155.h5 ./data/cX_000000-032768.h5 ./data/cX_032768-065536.h5 ./data/cX_065536-098304.h5 ./data/cX_098304-110875.h5 ./data/cY_000000-032768.h5 ./data/cY_032768-060021.h5 --val_files ./data/c1_000000-032768.h5 ./data/c1_032768-065536.h5 ./data/c1_065536-098304.h5 ./data/c1_098304-131072.h5 ./data/c1_131072-163840.h5 ./data/c1_163840-170735.h5 ./data/c2_000000-032768.h5 ./data/c2_032768-065536.h5 ./data/c2_065536-098304.h5 ./data/c2_098304-131072.h5 ./data/c2_131072-163840.h5 ./data/c2_163840-174991.h5 ./data/c3_000000-032768.h5 ./data/c3_032768-065536.h5 ./data/c3_065536-098304.h5 ./data/c3_098304-131072.h5 ./data/c3_131072-137553.h5 ./data/c4_000000-032768.h5 ./data/c4_032768-065536.h5 ./data/c4_065536-098304.h5 ./data/c4_098304-131072.h5 ./data/c4_131072-153643.h5 ./data/c5_000000-032768.h5 ./data/c5_032768-065536.h5 ./data/c5_065536-098304.h5 ./data/c5_098304-131072.h5 ./data/c5_131072-155221.h5 ./data/c6_000000-032768.h5 ./data/c6_032768-065536.h5 ./data/c6_065536-098304.h5 ./data/c6_098304-131072.h5 ./data/c6_131072-134244.h5 ./data/c7_000000-032768.h5 ./data/c7_032768-065536.h5 ./data/c7_065536-098304.h5 ./data/c7_098304-131072.h5 ./data/c7_131072-140587.h5 ./data/c8_000000-032768.h5 ./data/c8_032768-065536.h5 ./data/c8_065536-098304.h5 ./data/c8_098304-130549.h5 ./data/c9_000000-032768.h5 ./data/c9_032768-065536.h5 ./data/c9_065536-098304.h5 ./data/c9_098304-124155.h5 ./data/cX_000000-032768.h5 ./data/cX_032768-065536.h5 ./data/cX_065536-098304.h5 ./data/cX_098304-110875.h5 ./data/cY_000000-032768.h5 ./data/cY_032768-060021.h5 --out_dir ./models/dna --dna_model CnnL1h128 --nb_epoch 30
#################################
Using TensorFlow backend.
INFO (2017-07-25 20:36:34,533): Building model ...
INFO (2017-07-25 20:36:34,538): Building DNA model ...
INFO (2017-07-25 20:36:34,689): Computing output statistics ...
INFO (2017-07-25 20:36:36,439): Loading data ...
INFO (2017-07-25 20:36:36,542): Initializing callbacks ...
INFO (2017-07-25 20:36:36,542): Training model ...
2017-07-25 20:36:37.295323: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-25 20:36:37.295345: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-25 20:36:37.295348: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-25 20:36:37.295351: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-25 20:36:37.295359: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-25 20:36:37.535429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 960
major: 5 minor: 2 memoryClockRate (GHz) 1.1775
pciBusID 0000:02:00.0
Total memory: 1.95GiB
Free memory: 1.86GiB
2017-07-25 20:36:37.535460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2017-07-25 20:36:37.535465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2017-07-25 20:36:37.535473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:02:00.0)
2017-07-25 20:36:39.649197: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 8269 get requests, put_count=8260 evicted_count=1000 eviction_rate=0.121065 and unsatisfied allocation rate=0.134115
2017-07-25 20:36:39.649224: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dna (InputLayer)                 (None, 51, 4)         0                                            
____________________________________________________________________________________________________
dna/convolution1d_1 (Convolution (None, 41, 128)       5760        dna[0][0]                        
____________________________________________________________________________________________________
dna/activation_1 (Activation)    (None, 41, 128)       0           dna/convolution1d_1[0][0]        
____________________________________________________________________________________________________
dna/maxpooling1d_1 (MaxPooling1D (None, 10, 128)       0           dna/activation_1[0][0]           
____________________________________________________________________________________________________
dna/flatten_1 (Flatten)          (None, 1280)          0           dna/maxpooling1d_1[0][0]         
____________________________________________________________________________________________________
dna/dense_1 (Dense)              (None, 128)           163968      dna/flatten_1[0][0]              
____________________________________________________________________________________________________
dna/activation_2 (Activation)    (None, 128)           0           dna/dense_1[0][0]                
____________________________________________________________________________________________________
dna/dropout_1 (Dropout)          (None, 128)           0           dna/activation_2[0][0]           
____________________________________________________________________________________________________
cpg/1 (Dense)                    (None, 1)             129         dna/dropout_1[0][0]              
____________________________________________________________________________________________________
cpg/2 (Dense)                    (None, 1)             129         dna/dropout_1[0][0]              
____________________________________________________________________________________________________
cpg/3 (Dense)                    (None, 1)             129         dna/dropout_1[0][0]              
____________________________________________________________________________________________________
cpg/4 (Dense)                    (None, 1)             129         dna/dropout_1[0][0]              
____________________________________________________________________________________________________
cpg/5 (Dense)                    (None, 1)             129         dna/dropout_1[0][0]              
====================================================================================================
Total params: 170,373
Trainable params: 170,373
Non-trainable params: 0
____________________________________________________________________________________________________
Output statistics:
 name |  nb_tot | nb_obs | frac_obs | mean |  var
-------------------------------------------------
cpg/1 | 1492574 | 528951 |     0.35 | 0.48 | 0.25
cpg/2 | 1492574 | 529018 |     0.35 | 0.48 | 0.25
cpg/3 | 1492574 | 528858 |     0.35 | 0.48 | 0.25
cpg/4 | 1492574 | 528803 |     0.35 | 0.48 | 0.25
cpg/5 | 1492574 | 528826 |     0.35 | 0.48 | 0.25

Class weights:
 cpg/1 |  cpg/2 |  cpg/3 |  cpg/4 |  cpg/5
------------------------------------------
0=0.48 | 0=0.48 | 0=0.48 | 0=0.48 | 0=0.48
1=0.52 | 1=0.52 | 1=0.52 | 1=0.52 | 1=0.52


Training samples: 1492574
Validation samples: 1492574
Epochs: 30
Learning rate: 0.0001
====================================================================================================
Epoch 1/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 |  0.0 | 1.0865 | 0.5369 |     0.1016 |     0.1037 |     0.1105 |     0.0896 |     0.1143 |    0.6316 |    0.3939 |    0.3500 |    0.6512 |    0.6579
    10.0 |  0.2 | 0.7529 | 0.6813 |     0.1065 |     0.1066 |     0.1062 |     0.1055 |     0.1061 |    0.6802 |    0.6824 |    0.6819 |    0.6823 |    0.6798
    20.0 |  0.4 | 0.6196 | 0.7377 |     0.0933 |     0.0936 |     0.0933 |     0.0927 |     0.0933 |    0.7379 |    0.7387 |    0.7377 |    0.7376 |    0.7368
    30.0 |  0.6 | 0.5525 | 0.7702 |     0.0851 |     0.0852 |     0.0851 |     0.0845 |     0.0851 |    0.7704 |    0.7709 |    0.7703 |    0.7699 |    0.7695
    40.0 |  0.8 | 0.5133 | 0.7892 |     0.0801 |     0.0802 |     0.0799 |     0.0794 |     0.0799 |    0.7888 |    0.7899 |    0.7894 |    0.7892 |    0.7886
    50.0 |  1.0 | 0.4832 | 0.8047 |     0.0758 |     0.0758 |     0.0756 |     0.0751 |     0.0754 |    0.8041 |    0.8054 |    0.8050 |    0.8048 |    0.8041
    60.0 |  1.2 | 0.4594 | 0.8174 |     0.0720 |     0.0721 |     0.0720 |     0.0715 |     0.0718 |    0.8169 |    0.8180 |    0.8178 |    0.8176 |    0.8169
    70.0 |  1.4 | 0.4383 | 0.8291 |     0.0685 |     0.0687 |     0.0684 |     0.0680 |     0.0682 |    0.8287 |    0.8297 |    0.8295 |    0.8294 |    0.8285
    80.1 |  1.6 | 0.4211 | 0.8383 |     0.0656 |     0.0658 |     0.0656 |     0.0651 |     0.0654 |    0.8381 |    0.8389 |    0.8384 |    0.8385 |    0.8377
    90.1 |  1.8 | 0.4064 | 0.8463 |     0.0630 |     0.0632 |     0.0630 |     0.0627 |     0.0629 |    0.8462 |    0.8468 |    0.8463 |    0.8463 |    0.8458
   100.0 |  2.0 | 0.3935 | 0.8531 |     0.0608 |     0.0609 |     0.0609 |     0.0605 |     0.0607 |    0.8530 |    0.8536 |    0.8530 |    0.8530 |    0.8526
Epoch 00000: val_loss improved from inf to 0.46008, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.3935 | 0.8531 |     0.0608 |     0.0609 |     0.0607 |     0.0609 |     0.0605 |    0.8536 |    0.8526 |    0.8530 |    0.8530 |    0.8530
   val | 0.4601 | 0.9215 |     0.0773 |     0.0771 |     0.0771 |     0.0776 |     0.0772 |    0.9216 |    0.9214 |    0.9221 |    0.9214 |    0.9212
Learning rate: 9.75e-05
====================================================================================================
Epoch 2/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 |  3.2 | 0.1931 | 0.9720 |     0.0320 |     0.0300 |     0.0131 |     0.0260 |     0.0183 |    0.9286 |    0.9750 |    1.0000 |    0.9783 |    0.9783
    10.0 |  3.4 | 0.2619 | 0.9239 |     0.0374 |     0.0379 |     0.0380 |     0.0372 |     0.0373 |    0.9246 |    0.9247 |    0.9226 |    0.9241 |    0.9235
    20.0 |  3.6 | 0.2590 | 0.9250 |     0.0370 |     0.0371 |     0.0374 |     0.0369 |     0.0369 |    0.9248 |    0.9253 |    0.9243 |    0.9256 |    0.9249
    30.0 |  3.8 | 0.2497 | 0.9292 |     0.0353 |     0.0350 |     0.0353 |     0.0351 |     0.0349 |    0.9285 |    0.9295 |    0.9291 |    0.9298 |    0.9291
    40.0 |  4.0 | 0.2459 | 0.9307 |     0.0345 |     0.0343 |     0.0346 |     0.0344 |     0.0342 |    0.9302 |    0.9310 |    0.9308 |    0.9311 |    0.9307
    50.0 |  4.2 | 0.2411 | 0.9329 |     0.0335 |     0.0334 |     0.0337 |     0.0336 |     0.0334 |    0.9326 |    0.9330 |    0.9330 |    0.9330 |    0.9331
    60.0 |  4.4 | 0.2365 | 0.9352 |     0.0326 |     0.0325 |     0.0328 |     0.0327 |     0.0325 |    0.9350 |    0.9353 |    0.9353 |    0.9352 |    0.9352
    70.0 |  4.6 | 0.2331 | 0.9365 |     0.0320 |     0.0319 |     0.0321 |     0.0320 |     0.0319 |    0.9363 |    0.9366 |    0.9367 |    0.9365 |    0.9366
    80.0 |  4.8 | 0.2300 | 0.9378 |     0.0315 |     0.0314 |     0.0315 |     0.0314 |     0.0314 |    0.9375 |    0.9380 |    0.9381 |    0.9378 |    0.9378
    90.0 |  5.0 | 0.2271 | 0.9391 |     0.0309 |     0.0308 |     0.0310 |     0.0309 |     0.0309 |    0.9389 |    0.9393 |    0.9392 |    0.9390 |    0.9392
   100.0 |  5.2 | 0.2241 | 0.9404 |     0.0304 |     0.0303 |     0.0304 |     0.0304 |     0.0304 |    0.9402 |    0.9405 |    0.9404 |    0.9404 |    0.9404
Epoch 00001: val_loss improved from 0.46008 to 0.30839, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.2241 | 0.9404 |     0.0304 |     0.0303 |     0.0304 |     0.0304 |     0.0304 |    0.9405 |    0.9404 |    0.9404 |    0.9402 |    0.9404
   val | 0.3084 | 0.9555 |     0.0478 |     0.0477 |     0.0477 |     0.0478 |     0.0477 |    0.9557 |    0.9555 |    0.9557 |    0.9553 |    0.9555
Learning rate: 9.51e-05
====================================================================================================
Epoch 3/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 |  6.4 | 0.2400 | 0.9245 |     0.0502 |     0.0246 |     0.0372 |     0.0326 |     0.0258 |    0.9000 |    0.9048 |    0.9245 |    0.9348 |    0.9583
    10.0 |  6.6 | 0.1903 | 0.9553 |     0.0245 |     0.0238 |     0.0241 |     0.0244 |     0.0242 |    0.9543 |    0.9554 |    0.9554 |    0.9553 |    0.9562
    20.0 |  6.8 | 0.1892 | 0.9555 |     0.0241 |     0.0238 |     0.0242 |     0.0241 |     0.0240 |    0.9552 |    0.9555 |    0.9550 |    0.9558 |    0.9563
    30.0 |  7.0 | 0.1867 | 0.9563 |     0.0237 |     0.0233 |     0.0238 |     0.0238 |     0.0235 |    0.9559 |    0.9560 |    0.9558 |    0.9566 |    0.9572
    40.0 |  7.2 | 0.1835 | 0.9575 |     0.0231 |     0.0228 |     0.0232 |     0.0232 |     0.0229 |    0.9572 |    0.9573 |    0.9568 |    0.9577 |    0.9583
    50.0 |  7.4 | 0.1810 | 0.9581 |     0.0226 |     0.0225 |     0.0227 |     0.0227 |     0.0224 |    0.9578 |    0.9581 |    0.9576 |    0.9584 |    0.9587
    60.1 |  7.6 | 0.1792 | 0.9586 |     0.0223 |     0.0221 |     0.0225 |     0.0224 |     0.0222 |    0.9585 |    0.9585 |    0.9581 |    0.9587 |    0.9593
    70.1 |  7.8 | 0.1776 | 0.9592 |     0.0220 |     0.0219 |     0.0221 |     0.0221 |     0.0220 |    0.9591 |    0.9592 |    0.9588 |    0.9591 |    0.9596
    80.1 |  8.0 | 0.1756 | 0.9599 |     0.0217 |     0.0216 |     0.0218 |     0.0217 |     0.0217 |    0.9597 |    0.9599 |    0.9596 |    0.9598 |    0.9603
    90.1 |  8.2 | 0.1731 | 0.9608 |     0.0213 |     0.0211 |     0.0213 |     0.0213 |     0.0212 |    0.9606 |    0.9608 |    0.9606 |    0.9608 |    0.9611
   100.0 |  8.4 | 0.1708 | 0.9616 |     0.0209 |     0.0207 |     0.0209 |     0.0208 |     0.0208 |    0.9615 |    0.9617 |    0.9614 |    0.9616 |    0.9619
Epoch 00002: val_loss improved from 0.30839 to 0.24205, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.1708 | 0.9616 |     0.0209 |     0.0207 |     0.0208 |     0.0209 |     0.0208 |    0.9617 |    0.9619 |    0.9616 |    0.9615 |    0.9614
   val | 0.2421 | 0.9674 |     0.0357 |     0.0355 |     0.0357 |     0.0357 |     0.0357 |    0.9673 |    0.9676 |    0.9674 |    0.9673 |    0.9674
Learning rate: 9.27e-05
====================================================================================================
Epoch 4/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 |  9.6 | 0.1697 | 0.9689 |     0.0306 |     0.0175 |     0.0236 |     0.0200 |     0.0143 |    0.9318 |    0.9744 |    0.9583 |    1.0000 |    0.9800
    10.0 |  9.8 | 0.1539 | 0.9673 |     0.0179 |     0.0178 |     0.0186 |     0.0183 |     0.0179 |    0.9676 |    0.9667 |    0.9665 |    0.9679 |    0.9678
    20.0 | 10.0 | 0.1525 | 0.9679 |     0.0178 |     0.0177 |     0.0180 |     0.0180 |     0.0179 |    0.9680 |    0.9675 |    0.9676 |    0.9681 |    0.9682
    30.0 | 10.2 | 0.1504 | 0.9685 |     0.0175 |     0.0174 |     0.0177 |     0.0176 |     0.0175 |    0.9686 |    0.9681 |    0.9683 |    0.9688 |    0.9686
    40.0 | 10.4 | 0.1479 | 0.9695 |     0.0170 |     0.0170 |     0.0172 |     0.0172 |     0.0171 |    0.9697 |    0.9691 |    0.9694 |    0.9697 |    0.9697
    50.0 | 10.6 | 0.1467 | 0.9697 |     0.0169 |     0.0168 |     0.0170 |     0.0171 |     0.0168 |    0.9698 |    0.9693 |    0.9698 |    0.9700 |    0.9698
    60.0 | 10.8 | 0.1458 | 0.9700 |     0.0168 |     0.0167 |     0.0168 |     0.0169 |     0.0167 |    0.9699 |    0.9697 |    0.9700 |    0.9702 |    0.9702
    70.0 | 11.0 | 0.1443 | 0.9705 |     0.0166 |     0.0164 |     0.0166 |     0.0166 |     0.0165 |    0.9703 |    0.9704 |    0.9705 |    0.9706 |    0.9708
    80.1 | 11.2 | 0.1431 | 0.9708 |     0.0164 |     0.0163 |     0.0164 |     0.0164 |     0.0164 |    0.9708 |    0.9707 |    0.9708 |    0.9709 |    0.9710
    90.1 | 11.4 | 0.1421 | 0.9712 |     0.0162 |     0.0161 |     0.0163 |     0.0163 |     0.0162 |    0.9711 |    0.9711 |    0.9712 |    0.9713 |    0.9713
   100.0 | 11.6 | 0.1411 | 0.9715 |     0.0161 |     0.0160 |     0.0161 |     0.0161 |     0.0161 |    0.9714 |    0.9714 |    0.9715 |    0.9716 |    0.9716
Epoch 00003: val_loss improved from 0.24205 to 0.21403, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.1411 | 0.9715 |     0.0161 |     0.0160 |     0.0161 |     0.0161 |     0.0161 |    0.9714 |    0.9716 |    0.9716 |    0.9714 |    0.9715
   val | 0.2140 | 0.9716 |     0.0312 |     0.0312 |     0.0311 |     0.0313 |     0.0314 |    0.9713 |    0.9716 |    0.9718 |    0.9717 |    0.9715
Learning rate: 9.04e-05
====================================================================================================
Epoch 5/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 12.8 | 0.1806 | 0.9660 |     0.0228 |     0.0201 |     0.0192 |     0.0368 |     0.0237 |    0.9800 |    0.9333 |    0.9778 |    0.9608 |    0.9783
    10.0 | 13.0 | 0.1312 | 0.9744 |     0.0148 |     0.0145 |     0.0147 |     0.0146 |     0.0150 |    0.9737 |    0.9746 |    0.9748 |    0.9742 |    0.9748
    20.0 | 13.2 | 0.1289 | 0.9752 |     0.0143 |     0.0142 |     0.0143 |     0.0143 |     0.0144 |    0.9749 |    0.9754 |    0.9753 |    0.9753 |    0.9750
    30.0 | 13.4 | 0.1269 | 0.9760 |     0.0140 |     0.0138 |     0.0140 |     0.0139 |     0.0139 |    0.9757 |    0.9762 |    0.9760 |    0.9761 |    0.9758
    40.0 | 13.6 | 0.1261 | 0.9763 |     0.0139 |     0.0137 |     0.0139 |     0.0138 |     0.0138 |    0.9759 |    0.9766 |    0.9762 |    0.9766 |    0.9763
    50.0 | 13.8 | 0.1256 | 0.9764 |     0.0138 |     0.0136 |     0.0139 |     0.0137 |     0.0138 |    0.9762 |    0.9767 |    0.9762 |    0.9767 |    0.9764
    60.0 | 14.0 | 0.1246 | 0.9767 |     0.0137 |     0.0135 |     0.0138 |     0.0135 |     0.0135 |    0.9764 |    0.9770 |    0.9765 |    0.9771 |    0.9766
    70.0 | 14.2 | 0.1238 | 0.9770 |     0.0135 |     0.0134 |     0.0136 |     0.0135 |     0.0134 |    0.9768 |    0.9771 |    0.9768 |    0.9773 |    0.9769
    80.1 | 14.4 | 0.1230 | 0.9772 |     0.0134 |     0.0133 |     0.0135 |     0.0134 |     0.0133 |    0.9771 |    0.9773 |    0.9770 |    0.9774 |    0.9770
    90.1 | 14.6 | 0.1220 | 0.9775 |     0.0132 |     0.0132 |     0.0133 |     0.0132 |     0.0132 |    0.9775 |    0.9777 |    0.9774 |    0.9778 |    0.9774
   100.0 | 14.8 | 0.1213 | 0.9778 |     0.0131 |     0.0131 |     0.0132 |     0.0131 |     0.0131 |    0.9777 |    0.9778 |    0.9777 |    0.9779 |    0.9777
Epoch 00004: val_loss improved from 0.21403 to 0.17096, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.1213 | 0.9778 |     0.0131 |     0.0131 |     0.0131 |     0.0132 |     0.0131 |    0.9778 |    0.9777 |    0.9779 |    0.9777 |    0.9777
   val | 0.1710 | 0.9805 |     0.0235 |     0.0234 |     0.0234 |     0.0236 |     0.0235 |    0.9805 |    0.9805 |    0.9807 |    0.9805 |    0.9805
Learning rate: 8.81e-05
====================================================================================================
Epoch 6/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 16.0 | 0.1164 | 0.9817 |     0.0211 |     0.0040 |     0.0063 |     0.0085 |     0.0230 |    0.9535 |    0.9756 |    1.0000 |    0.9796 |    1.0000
    10.0 | 16.2 | 0.1131 | 0.9804 |     0.0117 |     0.0119 |     0.0121 |     0.0122 |     0.0119 |    0.9809 |    0.9803 |    0.9803 |    0.9804 |    0.9799
    20.0 | 16.4 | 0.1118 | 0.9808 |     0.0116 |     0.0116 |     0.0118 |     0.0119 |     0.0117 |    0.9811 |    0.9806 |    0.9808 |    0.9809 |    0.9807
    30.0 | 16.6 | 0.1115 | 0.9808 |     0.0117 |     0.0116 |     0.0118 |     0.0118 |     0.0117 |    0.9809 |    0.9806 |    0.9808 |    0.9810 |    0.9808
    40.0 | 16.8 | 0.1112 | 0.9808 |     0.0117 |     0.0116 |     0.0118 |     0.0117 |     0.0116 |    0.9809 |    0.9807 |    0.9808 |    0.9809 |    0.9808
    50.0 | 17.0 | 0.1109 | 0.9809 |     0.0117 |     0.0116 |     0.0117 |     0.0117 |     0.0116 |    0.9810 |    0.9809 |    0.9809 |    0.9810 |    0.9809
    60.0 | 17.2 | 0.1107 | 0.9809 |     0.0117 |     0.0116 |     0.0117 |     0.0117 |     0.0117 |    0.9810 |    0.9807 |    0.9809 |    0.9809 |    0.9809
    70.0 | 17.4 | 0.1100 | 0.9811 |     0.0115 |     0.0115 |     0.0116 |     0.0116 |     0.0116 |    0.9812 |    0.9810 |    0.9812 |    0.9811 |    0.9812
    80.1 | 17.6 | 0.1091 | 0.9814 |     0.0114 |     0.0114 |     0.0114 |     0.0114 |     0.0114 |    0.9814 |    0.9813 |    0.9815 |    0.9813 |    0.9814
    90.1 | 17.8 | 0.1087 | 0.9814 |     0.0114 |     0.0113 |     0.0114 |     0.0114 |     0.0113 |    0.9814 |    0.9813 |    0.9815 |    0.9815 |    0.9814
   100.0 | 18.0 | 0.1081 | 0.9816 |     0.0113 |     0.0112 |     0.0113 |     0.0113 |     0.0112 |    0.9815 |    0.9815 |    0.9817 |    0.9817 |    0.9817
Epoch 00005: val_loss improved from 0.17096 to 0.15372, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.1081 | 0.9816 |     0.0113 |     0.0112 |     0.0112 |     0.0113 |     0.0113 |    0.9815 |    0.9817 |    0.9817 |    0.9815 |    0.9817
   val | 0.1537 | 0.9834 |     0.0208 |     0.0206 |     0.0206 |     0.0209 |     0.0208 |    0.9834 |    0.9835 |    0.9836 |    0.9833 |    0.9833
Learning rate: 8.59e-05
====================================================================================================
Epoch 7/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 19.2 | 0.0898 | 0.9843 |     0.0099 |     0.0088 |     0.0104 |     0.0057 |     0.0050 |    0.9773 |    1.0000 |    0.9722 |    1.0000 |    0.9722
    10.0 | 19.4 | 0.1041 | 0.9828 |     0.0108 |     0.0109 |     0.0110 |     0.0107 |     0.0108 |    0.9823 |    0.9832 |    0.9825 |    0.9833 |    0.9826
    20.0 | 19.6 | 0.1025 | 0.9831 |     0.0104 |     0.0106 |     0.0108 |     0.0106 |     0.0105 |    0.9834 |    0.9831 |    0.9827 |    0.9832 |    0.9829
    30.0 | 19.8 | 0.1020 | 0.9833 |     0.0104 |     0.0105 |     0.0107 |     0.0105 |     0.0104 |    0.9835 |    0.9833 |    0.9831 |    0.9833 |    0.9831
    40.0 | 20.0 | 0.1005 | 0.9838 |     0.0102 |     0.0102 |     0.0104 |     0.0102 |     0.0102 |    0.9837 |    0.9840 |    0.9837 |    0.9838 |    0.9836
    50.0 | 20.2 | 0.1001 | 0.9838 |     0.0101 |     0.0102 |     0.0103 |     0.0102 |     0.0101 |    0.9837 |    0.9840 |    0.9838 |    0.9838 |    0.9837
    60.0 | 20.4 | 0.1001 | 0.9837 |     0.0102 |     0.0102 |     0.0103 |     0.0102 |     0.0102 |    0.9836 |    0.9838 |    0.9836 |    0.9836 |    0.9837
    70.0 | 20.6 | 0.0996 | 0.9838 |     0.0101 |     0.0101 |     0.0103 |     0.0102 |     0.0101 |    0.9837 |    0.9839 |    0.9837 |    0.9838 |    0.9839
    80.0 | 20.8 | 0.0994 | 0.9838 |     0.0101 |     0.0101 |     0.0103 |     0.0102 |     0.0101 |    0.9838 |    0.9839 |    0.9838 |    0.9838 |    0.9839
    90.1 | 21.0 | 0.0990 | 0.9839 |     0.0101 |     0.0100 |     0.0102 |     0.0102 |     0.0100 |    0.9838 |    0.9838 |    0.9838 |    0.9840 |    0.9839
   100.0 | 21.2 | 0.0986 | 0.9840 |     0.0101 |     0.0100 |     0.0101 |     0.0101 |     0.0100 |    0.9839 |    0.9840 |    0.9839 |    0.9840 |    0.9841
Epoch 00006: val_loss improved from 0.15372 to 0.14698, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0986 | 0.9840 |     0.0101 |     0.0100 |     0.0100 |     0.0101 |     0.0101 |    0.9840 |    0.9841 |    0.9840 |    0.9839 |    0.9839
   val | 0.1470 | 0.9840 |     0.0201 |     0.0200 |     0.0199 |     0.0201 |     0.0200 |    0.9840 |    0.9840 |    0.9841 |    0.9838 |    0.9839
Learning rate: 8.38e-05
====================================================================================================
Epoch 8/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 22.4 | 0.0860 | 0.9917 |     0.0058 |     0.0072 |     0.0050 |     0.0047 |     0.0164 |    1.0000 |    1.0000 |    1.0000 |    0.9583 |    1.0000
    10.0 | 22.6 | 0.0935 | 0.9851 |     0.0089 |     0.0093 |     0.0096 |     0.0094 |     0.0095 |    0.9855 |    0.9849 |    0.9848 |    0.9850 |    0.9854
    20.0 | 22.8 | 0.0941 | 0.9851 |     0.0094 |     0.0095 |     0.0096 |     0.0094 |     0.0095 |    0.9850 |    0.9851 |    0.9852 |    0.9850 |    0.9851
    30.0 | 23.0 | 0.0939 | 0.9850 |     0.0094 |     0.0095 |     0.0095 |     0.0095 |     0.0095 |    0.9850 |    0.9850 |    0.9851 |    0.9851 |    0.9850
    40.0 | 23.2 | 0.0937 | 0.9850 |     0.0094 |     0.0095 |     0.0095 |     0.0095 |     0.0094 |    0.9850 |    0.9849 |    0.9850 |    0.9853 |    0.9849
    50.0 | 23.4 | 0.0936 | 0.9850 |     0.0094 |     0.0095 |     0.0095 |     0.0095 |     0.0095 |    0.9850 |    0.9849 |    0.9851 |    0.9850 |    0.9850
    60.0 | 23.6 | 0.0935 | 0.9850 |     0.0094 |     0.0095 |     0.0095 |     0.0095 |     0.0094 |    0.9850 |    0.9849 |    0.9850 |    0.9851 |    0.9851
    70.1 | 23.8 | 0.0934 | 0.9850 |     0.0095 |     0.0095 |     0.0095 |     0.0095 |     0.0094 |    0.9849 |    0.9850 |    0.9850 |    0.9851 |    0.9850
    80.1 | 24.0 | 0.0930 | 0.9850 |     0.0095 |     0.0094 |     0.0095 |     0.0095 |     0.0094 |    0.9849 |    0.9850 |    0.9850 |    0.9851 |    0.9852
    90.1 | 24.2 | 0.0926 | 0.9851 |     0.0094 |     0.0093 |     0.0095 |     0.0094 |     0.0094 |    0.9851 |    0.9850 |    0.9851 |    0.9852 |    0.9853
   100.0 | 24.4 | 0.0922 | 0.9853 |     0.0093 |     0.0093 |     0.0094 |     0.0094 |     0.0093 |    0.9852 |    0.9853 |    0.9853 |    0.9854 |    0.9854
Epoch 00007: val_loss did not improve

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0922 | 0.9853 |     0.0093 |     0.0093 |     0.0093 |     0.0094 |     0.0094 |    0.9853 |    0.9854 |    0.9854 |    0.9852 |    0.9853
   val | 0.1530 | 0.9809 |     0.0217 |     0.0215 |     0.0215 |     0.0218 |     0.0215 |    0.9810 |    0.9810 |    0.9811 |    0.9807 |    0.9807
Learning rate: 8.17e-05
====================================================================================================
Epoch 9/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 25.6 | 0.1289 | 0.9756 |     0.0154 |     0.0159 |     0.0150 |     0.0210 |     0.0165 |    0.9800 |    0.9608 |    0.9796 |    0.9796 |    0.9783
    10.0 | 25.8 | 0.0902 | 0.9854 |     0.0091 |     0.0089 |     0.0094 |     0.0091 |     0.0092 |    0.9854 |    0.9859 |    0.9849 |    0.9852 |    0.9854
    20.0 | 26.0 | 0.0902 | 0.9852 |     0.0092 |     0.0091 |     0.0093 |     0.0091 |     0.0093 |    0.9853 |    0.9857 |    0.9850 |    0.9850 |    0.9851
    30.0 | 26.2 | 0.0893 | 0.9854 |     0.0090 |     0.0089 |     0.0092 |     0.0090 |     0.0091 |    0.9855 |    0.9857 |    0.9852 |    0.9853 |    0.9854
    40.0 | 26.4 | 0.0890 | 0.9856 |     0.0089 |     0.0090 |     0.0092 |     0.0090 |     0.0090 |    0.9857 |    0.9858 |    0.9854 |    0.9856 |    0.9854
    50.0 | 26.6 | 0.0885 | 0.9858 |     0.0089 |     0.0089 |     0.0090 |     0.0089 |     0.0089 |    0.9859 |    0.9859 |    0.9856 |    0.9857 |    0.9857
    60.0 | 26.8 | 0.0882 | 0.9859 |     0.0089 |     0.0089 |     0.0090 |     0.0089 |     0.0089 |    0.9859 |    0.9860 |    0.9857 |    0.9859 |    0.9858
    70.0 | 27.0 | 0.0880 | 0.9859 |     0.0089 |     0.0088 |     0.0090 |     0.0089 |     0.0089 |    0.9858 |    0.9861 |    0.9858 |    0.9859 |    0.9859
    80.1 | 27.2 | 0.0873 | 0.9862 |     0.0088 |     0.0087 |     0.0088 |     0.0087 |     0.0087 |    0.9860 |    0.9863 |    0.9860 |    0.9862 |    0.9862
    90.1 | 27.4 | 0.0871 | 0.9862 |     0.0087 |     0.0087 |     0.0088 |     0.0088 |     0.0087 |    0.9861 |    0.9863 |    0.9861 |    0.9862 |    0.9862
   100.0 | 27.5 | 0.0870 | 0.9862 |     0.0087 |     0.0087 |     0.0088 |     0.0088 |     0.0087 |    0.9861 |    0.9863 |    0.9861 |    0.9862 |    0.9863
Epoch 00008: val_loss improved from 0.14698 to 0.12480, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0870 | 0.9862 |     0.0087 |     0.0087 |     0.0087 |     0.0088 |     0.0088 |    0.9863 |    0.9863 |    0.9862 |    0.9861 |    0.9861
   val | 0.1248 | 0.9868 |     0.0165 |     0.0164 |     0.0164 |     0.0166 |     0.0166 |    0.9868 |    0.9869 |    0.9869 |    0.9867 |    0.9869
Learning rate: 7.96e-05
====================================================================================================
Epoch 10/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 28.8 | 0.0838 | 0.9849 |     0.0069 |     0.0194 |     0.0039 |     0.0034 |     0.0080 |    0.9778 |    1.0000 |    1.0000 |    0.9811 |    0.9655
    10.0 | 29.0 | 0.0829 | 0.9878 |     0.0084 |     0.0084 |     0.0078 |     0.0080 |     0.0079 |    0.9874 |    0.9883 |    0.9879 |    0.9879 |    0.9873
    20.0 | 29.2 | 0.0838 | 0.9871 |     0.0083 |     0.0084 |     0.0084 |     0.0083 |     0.0083 |    0.9871 |    0.9874 |    0.9869 |    0.9872 |    0.9870
    30.0 | 29.4 | 0.0838 | 0.9871 |     0.0083 |     0.0084 |     0.0084 |     0.0082 |     0.0083 |    0.9871 |    0.9874 |    0.9868 |    0.9872 |    0.9869
    40.0 | 29.5 | 0.0836 | 0.9870 |     0.0084 |     0.0083 |     0.0084 |     0.0082 |     0.0084 |    0.9869 |    0.9873 |    0.9869 |    0.9870 |    0.9871
    50.0 | 29.7 | 0.0833 | 0.9871 |     0.0083 |     0.0083 |     0.0084 |     0.0082 |     0.0082 |    0.9869 |    0.9873 |    0.9869 |    0.9872 |    0.9870
    60.0 | 29.9 | 0.0830 | 0.9871 |     0.0083 |     0.0082 |     0.0083 |     0.0082 |     0.0082 |    0.9870 |    0.9873 |    0.9870 |    0.9872 |    0.9871
    70.0 | 30.1 | 0.0828 | 0.9872 |     0.0083 |     0.0082 |     0.0083 |     0.0082 |     0.0081 |    0.9870 |    0.9873 |    0.9872 |    0.9873 |    0.9872
    80.0 | 30.3 | 0.0826 | 0.9872 |     0.0082 |     0.0082 |     0.0083 |     0.0082 |     0.0081 |    0.9871 |    0.9872 |    0.9871 |    0.9873 |    0.9872
    90.1 | 30.5 | 0.0824 | 0.9872 |     0.0082 |     0.0081 |     0.0082 |     0.0082 |     0.0081 |    0.9872 |    0.9872 |    0.9871 |    0.9873 |    0.9873
   100.0 | 30.7 | 0.0822 | 0.9873 |     0.0081 |     0.0081 |     0.0082 |     0.0082 |     0.0081 |    0.9872 |    0.9873 |    0.9872 |    0.9874 |    0.9873
Epoch 00009: val_loss improved from 0.12480 to 0.11954, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0822 | 0.9873 |     0.0081 |     0.0081 |     0.0081 |     0.0082 |     0.0082 |    0.9873 |    0.9873 |    0.9874 |    0.9872 |    0.9872
   val | 0.1195 | 0.9872 |     0.0158 |     0.0157 |     0.0157 |     0.0159 |     0.0158 |    0.9872 |    0.9873 |    0.9873 |    0.9870 |    0.9871
Learning rate: 7.76e-05
====================================================================================================
Epoch 11/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 32.0 | 0.0919 | 0.9802 |     0.0034 |     0.0247 |     0.0036 |     0.0185 |     0.0011 |    1.0000 |    0.9250 |    1.0000 |    1.0000 |    0.9762
    10.0 | 32.1 | 0.0798 | 0.9877 |     0.0078 |     0.0079 |     0.0078 |     0.0079 |     0.0078 |    0.9878 |    0.9877 |    0.9879 |    0.9876 |    0.9876
    20.0 | 32.3 | 0.0806 | 0.9876 |     0.0082 |     0.0080 |     0.0078 |     0.0080 |     0.0080 |    0.9872 |    0.9874 |    0.9879 |    0.9876 |    0.9879
    30.0 | 32.5 | 0.0808 | 0.9876 |     0.0082 |     0.0081 |     0.0079 |     0.0081 |     0.0081 |    0.9871 |    0.9874 |    0.9880 |    0.9875 |    0.9877
    40.0 | 32.7 | 0.0800 | 0.9877 |     0.0080 |     0.0079 |     0.0078 |     0.0080 |     0.0079 |    0.9875 |    0.9875 |    0.9880 |    0.9878 |    0.9879
    50.0 | 32.9 | 0.0793 | 0.9880 |     0.0078 |     0.0078 |     0.0077 |     0.0079 |     0.0077 |    0.9878 |    0.9877 |    0.9883 |    0.9881 |    0.9881
    60.0 | 33.1 | 0.0791 | 0.9881 |     0.0078 |     0.0077 |     0.0077 |     0.0078 |     0.0077 |    0.9879 |    0.9879 |    0.9882 |    0.9882 |    0.9882
    70.0 | 33.3 | 0.0788 | 0.9880 |     0.0078 |     0.0077 |     0.0077 |     0.0078 |     0.0076 |    0.9878 |    0.9879 |    0.9881 |    0.9882 |    0.9882
    80.1 | 33.5 | 0.0788 | 0.9880 |     0.0078 |     0.0077 |     0.0077 |     0.0078 |     0.0077 |    0.9879 |    0.9878 |    0.9881 |    0.9882 |    0.9881
    90.1 | 33.7 | 0.0785 | 0.9881 |     0.0077 |     0.0077 |     0.0077 |     0.0078 |     0.0076 |    0.9879 |    0.9879 |    0.9881 |    0.9882 |    0.9882
   100.0 | 33.9 | 0.0782 | 0.9881 |     0.0077 |     0.0076 |     0.0077 |     0.0077 |     0.0076 |    0.9880 |    0.9880 |    0.9881 |    0.9882 |    0.9882
Epoch 00010: val_loss improved from 0.11954 to 0.11354, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0782 | 0.9881 |     0.0077 |     0.0076 |     0.0076 |     0.0077 |     0.0077 |    0.9880 |    0.9882 |    0.9882 |    0.9880 |    0.9881
   val | 0.1135 | 0.9881 |     0.0150 |     0.0148 |     0.0148 |     0.0149 |     0.0150 |    0.9881 |    0.9882 |    0.9882 |    0.9880 |    0.9882
Learning rate: 7.57e-05
====================================================================================================
Epoch 12/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 35.1 | 0.0519 | 1.0000 |     0.0014 |     0.0015 |     0.0036 |     0.0017 |     0.0046 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 35.3 | 0.0769 | 0.9883 |     0.0075 |     0.0075 |     0.0077 |     0.0078 |     0.0077 |    0.9887 |    0.9877 |    0.9882 |    0.9881 |    0.9885
    20.0 | 35.5 | 0.0763 | 0.9884 |     0.0073 |     0.0073 |     0.0077 |     0.0076 |     0.0076 |    0.9887 |    0.9879 |    0.9883 |    0.9882 |    0.9887
    30.0 | 35.7 | 0.0760 | 0.9884 |     0.0073 |     0.0074 |     0.0076 |     0.0076 |     0.0076 |    0.9888 |    0.9881 |    0.9884 |    0.9884 |    0.9886
    40.0 | 35.9 | 0.0759 | 0.9885 |     0.0074 |     0.0074 |     0.0076 |     0.0075 |     0.0075 |    0.9885 |    0.9884 |    0.9885 |    0.9884 |    0.9887
    50.0 | 36.1 | 0.0760 | 0.9884 |     0.0074 |     0.0074 |     0.0076 |     0.0075 |     0.0075 |    0.9884 |    0.9882 |    0.9885 |    0.9884 |    0.9886
    60.0 | 36.3 | 0.0758 | 0.9885 |     0.0075 |     0.0074 |     0.0076 |     0.0075 |     0.0075 |    0.9884 |    0.9884 |    0.9885 |    0.9885 |    0.9886
    70.0 | 36.5 | 0.0756 | 0.9886 |     0.0074 |     0.0074 |     0.0075 |     0.0075 |     0.0075 |    0.9885 |    0.9885 |    0.9887 |    0.9886 |    0.9887
    80.0 | 36.7 | 0.0752 | 0.9888 |     0.0074 |     0.0074 |     0.0074 |     0.0074 |     0.0073 |    0.9886 |    0.9887 |    0.9888 |    0.9888 |    0.9889
    90.1 | 36.9 | 0.0752 | 0.9887 |     0.0074 |     0.0074 |     0.0074 |     0.0074 |     0.0073 |    0.9886 |    0.9887 |    0.9888 |    0.9889 |    0.9888
   100.0 | 37.1 | 0.0751 | 0.9887 |     0.0074 |     0.0073 |     0.0074 |     0.0074 |     0.0073 |    0.9886 |    0.9886 |    0.9888 |    0.9888 |    0.9888
Epoch 00011: val_loss improved from 0.11354 to 0.10860, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0751 | 0.9887 |     0.0074 |     0.0073 |     0.0073 |     0.0074 |     0.0074 |    0.9886 |    0.9888 |    0.9888 |    0.9886 |    0.9888
   val | 0.1086 | 0.9889 |     0.0143 |     0.0142 |     0.0141 |     0.0143 |     0.0143 |    0.9888 |    0.9888 |    0.9889 |    0.9888 |    0.9889
Learning rate: 7.38e-05
====================================================================================================
Epoch 13/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 38.3 | 0.0627 | 0.9910 |     0.0015 |     0.0127 |     0.0082 |     0.0019 |     0.0011 |    1.0000 |    1.0000 |    0.9783 |    1.0000 |    0.9767
    10.0 | 38.5 | 0.0724 | 0.9896 |     0.0069 |     0.0072 |     0.0066 |     0.0071 |     0.0069 |    0.9894 |    0.9895 |    0.9905 |    0.9894 |    0.9894
    20.0 | 38.7 | 0.0728 | 0.9892 |     0.0070 |     0.0071 |     0.0068 |     0.0072 |     0.0069 |    0.9890 |    0.9890 |    0.9899 |    0.9893 |    0.9890
    30.0 | 38.9 | 0.0730 | 0.9892 |     0.0071 |     0.0072 |     0.0070 |     0.0072 |     0.0070 |    0.9890 |    0.9891 |    0.9895 |    0.9891 |    0.9890
    40.0 | 39.1 | 0.0729 | 0.9891 |     0.0071 |     0.0072 |     0.0070 |     0.0071 |     0.0070 |    0.9888 |    0.9892 |    0.9894 |    0.9892 |    0.9891
    50.0 | 39.3 | 0.0730 | 0.9891 |     0.0071 |     0.0071 |     0.0071 |     0.0072 |     0.0071 |    0.9889 |    0.9890 |    0.9892 |    0.9892 |    0.9892
    60.0 | 39.5 | 0.0728 | 0.9891 |     0.0072 |     0.0071 |     0.0071 |     0.0072 |     0.0070 |    0.9888 |    0.9890 |    0.9892 |    0.9893 |    0.9892
    70.0 | 39.7 | 0.0727 | 0.9891 |     0.0072 |     0.0071 |     0.0071 |     0.0072 |     0.0070 |    0.9889 |    0.9891 |    0.9893 |    0.9893 |    0.9891
    80.0 | 39.9 | 0.0724 | 0.9892 |     0.0071 |     0.0071 |     0.0071 |     0.0072 |     0.0070 |    0.9891 |    0.9891 |    0.9893 |    0.9894 |    0.9892
    90.1 | 40.1 | 0.0722 | 0.9892 |     0.0070 |     0.0070 |     0.0071 |     0.0071 |     0.0070 |    0.9892 |    0.9891 |    0.9893 |    0.9893 |    0.9893
   100.0 | 40.3 | 0.0722 | 0.9893 |     0.0071 |     0.0070 |     0.0071 |     0.0071 |     0.0070 |    0.9892 |    0.9892 |    0.9894 |    0.9893 |    0.9893
Epoch 00012: val_loss improved from 0.10860 to 0.10604, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0722 | 0.9893 |     0.0071 |     0.0070 |     0.0070 |     0.0071 |     0.0071 |    0.9892 |    0.9893 |    0.9893 |    0.9892 |    0.9894
   val | 0.1060 | 0.9899 |     0.0140 |     0.0138 |     0.0138 |     0.0140 |     0.0141 |    0.9898 |    0.9900 |    0.9899 |    0.9899 |    0.9900
Learning rate: 7.2e-05
====================================================================================================
Epoch 14/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 41.5 | 0.0590 | 1.0000 |     0.0036 |     0.0042 |     0.0048 |     0.0036 |     0.0065 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 41.7 | 0.0704 | 0.9896 |     0.0069 |     0.0066 |     0.0068 |     0.0068 |     0.0069 |    0.9900 |    0.9896 |    0.9894 |    0.9893 |    0.9896
    20.0 | 41.9 | 0.0704 | 0.9897 |     0.0069 |     0.0068 |     0.0067 |     0.0070 |     0.0068 |    0.9899 |    0.9894 |    0.9901 |    0.9894 |    0.9896
    30.0 | 42.1 | 0.0706 | 0.9895 |     0.0070 |     0.0069 |     0.0068 |     0.0070 |     0.0068 |    0.9896 |    0.9893 |    0.9899 |    0.9895 |    0.9894
    40.0 | 42.3 | 0.0706 | 0.9895 |     0.0070 |     0.0069 |     0.0068 |     0.0069 |     0.0068 |    0.9894 |    0.9895 |    0.9897 |    0.9895 |    0.9893
    50.0 | 42.5 | 0.0706 | 0.9894 |     0.0070 |     0.0069 |     0.0068 |     0.0069 |     0.0068 |    0.9894 |    0.9894 |    0.9897 |    0.9894 |    0.9893
    60.0 | 42.7 | 0.0704 | 0.9895 |     0.0069 |     0.0068 |     0.0068 |     0.0069 |     0.0068 |    0.9895 |    0.9893 |    0.9897 |    0.9894 |    0.9894
    70.0 | 42.9 | 0.0703 | 0.9895 |     0.0069 |     0.0068 |     0.0068 |     0.0069 |     0.0068 |    0.9894 |    0.9894 |    0.9897 |    0.9894 |    0.9894
    80.0 | 43.1 | 0.0703 | 0.9895 |     0.0069 |     0.0068 |     0.0069 |     0.0069 |     0.0068 |    0.9894 |    0.9894 |    0.9897 |    0.9894 |    0.9895
    90.1 | 43.3 | 0.0701 | 0.9896 |     0.0068 |     0.0068 |     0.0068 |     0.0069 |     0.0068 |    0.9896 |    0.9894 |    0.9897 |    0.9895 |    0.9896
   100.0 | 43.5 | 0.0699 | 0.9896 |     0.0068 |     0.0067 |     0.0068 |     0.0068 |     0.0067 |    0.9896 |    0.9895 |    0.9898 |    0.9897 |    0.9896
Epoch 00013: val_loss improved from 0.10604 to 0.10226, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0699 | 0.9896 |     0.0068 |     0.0067 |     0.0067 |     0.0068 |     0.0068 |    0.9895 |    0.9896 |    0.9897 |    0.9896 |    0.9898
   val | 0.1023 | 0.9900 |     0.0134 |     0.0132 |     0.0132 |     0.0134 |     0.0135 |    0.9899 |    0.9901 |    0.9900 |    0.9901 |    0.9901
Learning rate: 7.02e-05
====================================================================================================
Epoch 15/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 44.7 | 0.0512 | 0.9944 |     0.0021 |     0.0018 |     0.0030 |     0.0060 |     0.0028 |    1.0000 |    0.9722 |    1.0000 |    1.0000 |    1.0000
    10.0 | 44.9 | 0.0697 | 0.9896 |     0.0068 |     0.0067 |     0.0069 |     0.0068 |     0.0069 |    0.9901 |    0.9891 |    0.9896 |    0.9893 |    0.9899
    20.0 | 45.1 | 0.0689 | 0.9898 |     0.0067 |     0.0066 |     0.0068 |     0.0066 |     0.0067 |    0.9900 |    0.9897 |    0.9897 |    0.9896 |    0.9898
    30.0 | 45.3 | 0.0688 | 0.9899 |     0.0068 |     0.0066 |     0.0067 |     0.0066 |     0.0066 |    0.9899 |    0.9898 |    0.9898 |    0.9898 |    0.9899
    40.0 | 45.5 | 0.0686 | 0.9899 |     0.0067 |     0.0066 |     0.0067 |     0.0066 |     0.0065 |    0.9899 |    0.9898 |    0.9898 |    0.9899 |    0.9899
    50.0 | 45.7 | 0.0683 | 0.9900 |     0.0066 |     0.0065 |     0.0067 |     0.0066 |     0.0066 |    0.9900 |    0.9898 |    0.9899 |    0.9900 |    0.9900
    60.0 | 45.9 | 0.0683 | 0.9899 |     0.0066 |     0.0066 |     0.0067 |     0.0066 |     0.0066 |    0.9900 |    0.9898 |    0.9899 |    0.9899 |    0.9899
    70.0 | 46.1 | 0.0683 | 0.9899 |     0.0066 |     0.0065 |     0.0067 |     0.0067 |     0.0065 |    0.9900 |    0.9897 |    0.9899 |    0.9899 |    0.9899
    80.1 | 46.3 | 0.0683 | 0.9899 |     0.0066 |     0.0065 |     0.0067 |     0.0067 |     0.0066 |    0.9899 |    0.9897 |    0.9898 |    0.9899 |    0.9900
    90.1 | 46.5 | 0.0682 | 0.9899 |     0.0066 |     0.0065 |     0.0067 |     0.0067 |     0.0065 |    0.9899 |    0.9897 |    0.9898 |    0.9899 |    0.9900
   100.0 | 46.7 | 0.0682 | 0.9899 |     0.0066 |     0.0066 |     0.0066 |     0.0067 |     0.0065 |    0.9898 |    0.9898 |    0.9899 |    0.9899 |    0.9899
Epoch 00014: val_loss improved from 0.10226 to 0.10029, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0682 | 0.9899 |     0.0066 |     0.0066 |     0.0065 |     0.0066 |     0.0067 |    0.9898 |    0.9899 |    0.9899 |    0.9898 |    0.9899
   val | 0.1003 | 0.9901 |     0.0131 |     0.0129 |     0.0129 |     0.0130 |     0.0131 |    0.9899 |    0.9902 |    0.9901 |    0.9901 |    0.9901
Learning rate: 6.84e-05
====================================================================================================
Epoch 16/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 47.9 | 0.0730 | 0.9913 |     0.0104 |     0.0054 |     0.0069 |     0.0037 |     0.0113 |    0.9804 |    1.0000 |    1.0000 |    0.9762 |    1.0000
    10.0 | 48.1 | 0.0674 | 0.9904 |     0.0066 |     0.0065 |     0.0064 |     0.0066 |     0.0064 |    0.9903 |    0.9900 |    0.9908 |    0.9906 |    0.9901
    20.0 | 48.3 | 0.0663 | 0.9907 |     0.0063 |     0.0064 |     0.0062 |     0.0064 |     0.0062 |    0.9908 |    0.9904 |    0.9910 |    0.9908 |    0.9904
    30.0 | 48.5 | 0.0671 | 0.9904 |     0.0066 |     0.0064 |     0.0064 |     0.0065 |     0.0064 |    0.9903 |    0.9901 |    0.9906 |    0.9905 |    0.9902
    40.0 | 48.7 | 0.0672 | 0.9903 |     0.0066 |     0.0064 |     0.0064 |     0.0066 |     0.0064 |    0.9903 |    0.9901 |    0.9905 |    0.9904 |    0.9903
    50.0 | 48.9 | 0.0670 | 0.9903 |     0.0065 |     0.0064 |     0.0064 |     0.0065 |     0.0064 |    0.9902 |    0.9901 |    0.9904 |    0.9904 |    0.9903
    60.0 | 49.1 | 0.0670 | 0.9902 |     0.0065 |     0.0064 |     0.0065 |     0.0065 |     0.0064 |    0.9902 |    0.9900 |    0.9903 |    0.9903 |    0.9903
    70.0 | 49.3 | 0.0668 | 0.9903 |     0.0065 |     0.0064 |     0.0064 |     0.0065 |     0.0064 |    0.9903 |    0.9901 |    0.9905 |    0.9903 |    0.9904
    80.1 | 49.5 | 0.0668 | 0.9903 |     0.0065 |     0.0064 |     0.0064 |     0.0065 |     0.0064 |    0.9902 |    0.9901 |    0.9904 |    0.9903 |    0.9903
    90.1 | 49.7 | 0.0669 | 0.9902 |     0.0065 |     0.0064 |     0.0065 |     0.0065 |     0.0064 |    0.9902 |    0.9901 |    0.9903 |    0.9903 |    0.9903
   100.0 | 49.9 | 0.0668 | 0.9902 |     0.0065 |     0.0064 |     0.0065 |     0.0065 |     0.0064 |    0.9901 |    0.9901 |    0.9903 |    0.9902 |    0.9903
Epoch 00015: val_loss improved from 0.10029 to 0.09758, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0668 | 0.9902 |     0.0065 |     0.0064 |     0.0064 |     0.0065 |     0.0065 |    0.9901 |    0.9903 |    0.9902 |    0.9901 |    0.9903
   val | 0.0976 | 0.9905 |     0.0127 |     0.0126 |     0.0125 |     0.0127 |     0.0128 |    0.9903 |    0.9906 |    0.9905 |    0.9905 |    0.9906
Learning rate: 6.67e-05
====================================================================================================
Epoch 17/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 51.1 | 0.0541 | 0.9896 |     0.0058 |     0.0016 |     0.0055 |     0.0037 |     0.0031 |    0.9730 |    1.0000 |    0.9750 |    1.0000 |    1.0000
    10.0 | 51.3 | 0.0651 | 0.9906 |     0.0062 |     0.0061 |     0.0061 |     0.0063 |     0.0060 |    0.9906 |    0.9906 |    0.9909 |    0.9903 |    0.9906
    20.0 | 51.5 | 0.0654 | 0.9904 |     0.0061 |     0.0062 |     0.0062 |     0.0063 |     0.0062 |    0.9907 |    0.9902 |    0.9905 |    0.9902 |    0.9904
    30.0 | 51.7 | 0.0653 | 0.9906 |     0.0061 |     0.0062 |     0.0062 |     0.0063 |     0.0061 |    0.9907 |    0.9904 |    0.9907 |    0.9906 |    0.9905
    40.0 | 51.9 | 0.0655 | 0.9904 |     0.0062 |     0.0063 |     0.0063 |     0.0063 |     0.0062 |    0.9906 |    0.9904 |    0.9904 |    0.9905 |    0.9904
    50.0 | 52.1 | 0.0655 | 0.9905 |     0.0062 |     0.0063 |     0.0063 |     0.0063 |     0.0062 |    0.9905 |    0.9904 |    0.9906 |    0.9904 |    0.9905
    60.0 | 52.3 | 0.0656 | 0.9904 |     0.0063 |     0.0063 |     0.0063 |     0.0063 |     0.0062 |    0.9904 |    0.9904 |    0.9905 |    0.9903 |    0.9904
    70.0 | 52.5 | 0.0657 | 0.9904 |     0.0063 |     0.0063 |     0.0063 |     0.0063 |     0.0063 |    0.9904 |    0.9904 |    0.9904 |    0.9904 |    0.9903
    80.0 | 52.7 | 0.0656 | 0.9904 |     0.0063 |     0.0063 |     0.0063 |     0.0064 |     0.0062 |    0.9904 |    0.9904 |    0.9905 |    0.9904 |    0.9904
    90.1 | 52.9 | 0.0655 | 0.9904 |     0.0063 |     0.0063 |     0.0063 |     0.0063 |     0.0062 |    0.9904 |    0.9904 |    0.9905 |    0.9904 |    0.9905
   100.0 | 53.1 | 0.0656 | 0.9904 |     0.0063 |     0.0063 |     0.0063 |     0.0063 |     0.0062 |    0.9904 |    0.9903 |    0.9905 |    0.9904 |    0.9905
Epoch 00016: val_loss improved from 0.09758 to 0.09550, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0656 | 0.9904 |     0.0063 |     0.0063 |     0.0062 |     0.0063 |     0.0063 |    0.9903 |    0.9905 |    0.9904 |    0.9904 |    0.9905
   val | 0.0955 | 0.9901 |     0.0124 |     0.0123 |     0.0122 |     0.0124 |     0.0124 |    0.9900 |    0.9901 |    0.9902 |    0.9899 |    0.9903
Learning rate: 6.5e-05
====================================================================================================
Epoch 18/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 54.3 | 0.0694 | 0.9868 |     0.0042 |     0.0021 |     0.0070 |     0.0172 |     0.0051 |    1.0000 |    0.9767 |    0.9828 |    0.9744 |    1.0000
    10.0 | 54.5 | 0.0640 | 0.9905 |     0.0058 |     0.0061 |     0.0061 |     0.0062 |     0.0061 |    0.9905 |    0.9905 |    0.9907 |    0.9902 |    0.9908
    20.0 | 54.7 | 0.0640 | 0.9907 |     0.0058 |     0.0061 |     0.0061 |     0.0061 |     0.0061 |    0.9910 |    0.9907 |    0.9909 |    0.9903 |    0.9907
    30.0 | 54.9 | 0.0644 | 0.9907 |     0.0060 |     0.0062 |     0.0062 |     0.0061 |     0.0061 |    0.9907 |    0.9907 |    0.9907 |    0.9905 |    0.9908
    40.0 | 55.1 | 0.0642 | 0.9907 |     0.0060 |     0.0061 |     0.0062 |     0.0061 |     0.0061 |    0.9907 |    0.9907 |    0.9908 |    0.9905 |    0.9909
    50.0 | 55.3 | 0.0642 | 0.9907 |     0.0060 |     0.0061 |     0.0062 |     0.0061 |     0.0061 |    0.9906 |    0.9907 |    0.9907 |    0.9906 |    0.9909
    60.0 | 55.5 | 0.0641 | 0.9908 |     0.0060 |     0.0061 |     0.0061 |     0.0061 |     0.0061 |    0.9907 |    0.9907 |    0.9908 |    0.9907 |    0.9908
    70.0 | 55.7 | 0.0642 | 0.9907 |     0.0061 |     0.0061 |     0.0061 |     0.0061 |     0.0061 |    0.9906 |    0.9907 |    0.9909 |    0.9906 |    0.9908
    80.1 | 55.9 | 0.0642 | 0.9907 |     0.0061 |     0.0061 |     0.0061 |     0.0061 |     0.0061 |    0.9906 |    0.9906 |    0.9908 |    0.9906 |    0.9908
    90.1 | 56.1 | 0.0643 | 0.9907 |     0.0061 |     0.0061 |     0.0061 |     0.0062 |     0.0061 |    0.9906 |    0.9906 |    0.9908 |    0.9907 |    0.9908
   100.0 | 56.3 | 0.0643 | 0.9907 |     0.0062 |     0.0061 |     0.0062 |     0.0062 |     0.0061 |    0.9906 |    0.9906 |    0.9908 |    0.9907 |    0.9908
Epoch 00017: val_loss improved from 0.09550 to 0.09362, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0643 | 0.9907 |     0.0062 |     0.0061 |     0.0061 |     0.0062 |     0.0062 |    0.9906 |    0.9908 |    0.9907 |    0.9906 |    0.9908
   val | 0.0936 | 0.9903 |     0.0121 |     0.0120 |     0.0120 |     0.0121 |     0.0122 |    0.9902 |    0.9904 |    0.9904 |    0.9903 |    0.9904
Learning rate: 6.34e-05
====================================================================================================
Epoch 19/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 57.5 | 0.0537 | 0.9914 |     0.0014 |     0.0026 |     0.0076 |     0.0033 |     0.0056 |    1.0000 |    1.0000 |    0.9792 |    0.9778 |    1.0000
    10.0 | 57.7 | 0.0631 | 0.9910 |     0.0062 |     0.0059 |     0.0060 |     0.0059 |     0.0058 |    0.9906 |    0.9911 |    0.9912 |    0.9908 |    0.9911
    20.0 | 57.9 | 0.0634 | 0.9910 |     0.0062 |     0.0059 |     0.0060 |     0.0060 |     0.0060 |    0.9907 |    0.9909 |    0.9912 |    0.9908 |    0.9912
    30.0 | 58.1 | 0.0630 | 0.9911 |     0.0060 |     0.0058 |     0.0060 |     0.0060 |     0.0059 |    0.9909 |    0.9910 |    0.9912 |    0.9911 |    0.9914
    40.0 | 58.3 | 0.0633 | 0.9910 |     0.0061 |     0.0058 |     0.0061 |     0.0061 |     0.0060 |    0.9907 |    0.9909 |    0.9910 |    0.9911 |    0.9914
    50.0 | 58.5 | 0.0633 | 0.9910 |     0.0060 |     0.0059 |     0.0061 |     0.0061 |     0.0060 |    0.9908 |    0.9909 |    0.9910 |    0.9909 |    0.9913
    60.0 | 58.7 | 0.0631 | 0.9910 |     0.0060 |     0.0058 |     0.0060 |     0.0060 |     0.0059 |    0.9908 |    0.9909 |    0.9910 |    0.9910 |    0.9913
    70.0 | 58.9 | 0.0631 | 0.9910 |     0.0060 |     0.0059 |     0.0060 |     0.0060 |     0.0059 |    0.9908 |    0.9909 |    0.9910 |    0.9910 |    0.9912
    80.0 | 59.1 | 0.0632 | 0.9909 |     0.0060 |     0.0060 |     0.0060 |     0.0061 |     0.0060 |    0.9908 |    0.9908 |    0.9910 |    0.9909 |    0.9911
    90.1 | 59.3 | 0.0633 | 0.9909 |     0.0061 |     0.0060 |     0.0060 |     0.0061 |     0.0060 |    0.9908 |    0.9908 |    0.9909 |    0.9909 |    0.9910
   100.0 | 59.5 | 0.0633 | 0.9909 |     0.0061 |     0.0060 |     0.0060 |     0.0061 |     0.0060 |    0.9907 |    0.9908 |    0.9909 |    0.9909 |    0.9910
Epoch 00018: val_loss improved from 0.09362 to 0.09227, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0633 | 0.9909 |     0.0061 |     0.0060 |     0.0060 |     0.0060 |     0.0061 |    0.9908 |    0.9910 |    0.9909 |    0.9907 |    0.9909
   val | 0.0923 | 0.9907 |     0.0119 |     0.0118 |     0.0118 |     0.0119 |     0.0120 |    0.9906 |    0.9908 |    0.9907 |    0.9906 |    0.9908
Learning rate: 6.18e-05
====================================================================================================
Epoch 20/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 60.7 | 0.0475 | 1.0000 |     0.0060 |     0.0020 |     0.0020 |     0.0031 |     0.0015 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 60.9 | 0.0626 | 0.9908 |     0.0061 |     0.0059 |     0.0061 |     0.0060 |     0.0057 |    0.9904 |    0.9908 |    0.9905 |    0.9912 |    0.9911
    20.0 | 61.1 | 0.0629 | 0.9908 |     0.0062 |     0.0058 |     0.0062 |     0.0060 |     0.0058 |    0.9903 |    0.9910 |    0.9905 |    0.9911 |    0.9911
    30.0 | 61.3 | 0.0629 | 0.9909 |     0.0062 |     0.0059 |     0.0061 |     0.0060 |     0.0059 |    0.9905 |    0.9910 |    0.9908 |    0.9910 |    0.9911
    40.0 | 61.5 | 0.0629 | 0.9909 |     0.0061 |     0.0059 |     0.0060 |     0.0060 |     0.0060 |    0.9905 |    0.9909 |    0.9909 |    0.9909 |    0.9911
    50.0 | 61.7 | 0.0629 | 0.9908 |     0.0062 |     0.0059 |     0.0061 |     0.0061 |     0.0060 |    0.9905 |    0.9908 |    0.9908 |    0.9910 |    0.9912
    60.0 | 61.9 | 0.0627 | 0.9909 |     0.0061 |     0.0059 |     0.0060 |     0.0060 |     0.0059 |    0.9907 |    0.9909 |    0.9909 |    0.9910 |    0.9911
    70.0 | 62.1 | 0.0627 | 0.9909 |     0.0060 |     0.0059 |     0.0060 |     0.0060 |     0.0059 |    0.9907 |    0.9910 |    0.9909 |    0.9909 |    0.9911
    80.1 | 62.3 | 0.0627 | 0.9909 |     0.0060 |     0.0059 |     0.0060 |     0.0060 |     0.0059 |    0.9908 |    0.9910 |    0.9909 |    0.9909 |    0.9911
    90.1 | 62.4 | 0.0626 | 0.9909 |     0.0060 |     0.0059 |     0.0060 |     0.0060 |     0.0059 |    0.9908 |    0.9909 |    0.9909 |    0.9910 |    0.9911
   100.0 | 62.6 | 0.0625 | 0.9909 |     0.0060 |     0.0059 |     0.0060 |     0.0060 |     0.0059 |    0.9908 |    0.9909 |    0.9910 |    0.9910 |    0.9911
Epoch 00019: val_loss improved from 0.09227 to 0.09173, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0625 | 0.9909 |     0.0060 |     0.0059 |     0.0059 |     0.0060 |     0.0060 |    0.9909 |    0.9911 |    0.9910 |    0.9908 |    0.9910
   val | 0.0917 | 0.9912 |     0.0119 |     0.0118 |     0.0117 |     0.0119 |     0.0120 |    0.9911 |    0.9913 |    0.9912 |    0.9911 |    0.9912
Learning rate: 6.03e-05
====================================================================================================
Epoch 21/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 63.9 | 0.0496 | 1.0000 |     0.0017 |     0.0019 |     0.0044 |     0.0052 |     0.0038 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 64.1 | 0.0623 | 0.9913 |     0.0061 |     0.0061 |     0.0058 |     0.0061 |     0.0057 |    0.9911 |    0.9913 |    0.9914 |    0.9917 |    0.9908
    20.0 | 64.3 | 0.0624 | 0.9910 |     0.0061 |     0.0060 |     0.0059 |     0.0061 |     0.0057 |    0.9908 |    0.9910 |    0.9910 |    0.9914 |    0.9910
    30.0 | 64.5 | 0.0624 | 0.9911 |     0.0061 |     0.0059 |     0.0061 |     0.0060 |     0.0058 |    0.9909 |    0.9912 |    0.9910 |    0.9913 |    0.9911
    40.0 | 64.6 | 0.0624 | 0.9910 |     0.0061 |     0.0059 |     0.0061 |     0.0060 |     0.0059 |    0.9908 |    0.9911 |    0.9910 |    0.9911 |    0.9911
    50.0 | 64.9 | 0.0621 | 0.9911 |     0.0059 |     0.0059 |     0.0060 |     0.0059 |     0.0059 |    0.9911 |    0.9910 |    0.9910 |    0.9911 |    0.9912
    60.0 | 65.0 | 0.0622 | 0.9910 |     0.0059 |     0.0059 |     0.0061 |     0.0060 |     0.0059 |    0.9910 |    0.9910 |    0.9909 |    0.9911 |    0.9911
    70.1 | 65.2 | 0.0621 | 0.9910 |     0.0059 |     0.0058 |     0.0060 |     0.0060 |     0.0059 |    0.9910 |    0.9909 |    0.9909 |    0.9911 |    0.9912
    80.1 | 65.4 | 0.0618 | 0.9911 |     0.0059 |     0.0058 |     0.0059 |     0.0059 |     0.0058 |    0.9911 |    0.9911 |    0.9911 |    0.9912 |    0.9913
    90.1 | 65.6 | 0.0618 | 0.9911 |     0.0059 |     0.0058 |     0.0059 |     0.0059 |     0.0058 |    0.9910 |    0.9910 |    0.9911 |    0.9912 |    0.9913
   100.0 | 65.8 | 0.0617 | 0.9911 |     0.0059 |     0.0058 |     0.0059 |     0.0059 |     0.0058 |    0.9910 |    0.9910 |    0.9912 |    0.9911 |    0.9913
Epoch 00020: val_loss improved from 0.09173 to 0.09020, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0617 | 0.9911 |     0.0059 |     0.0058 |     0.0058 |     0.0059 |     0.0059 |    0.9910 |    0.9913 |    0.9911 |    0.9910 |    0.9912
   val | 0.0902 | 0.9914 |     0.0117 |     0.0115 |     0.0115 |     0.0116 |     0.0117 |    0.9912 |    0.9915 |    0.9913 |    0.9913 |    0.9914
Learning rate: 5.88e-05
====================================================================================================
Epoch 22/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 67.1 | 0.0859 | 0.9828 |     0.0038 |     0.0180 |     0.0035 |     0.0145 |     0.0139 |    1.0000 |    0.9767 |    1.0000 |    0.9783 |    0.9592
    10.0 | 67.2 | 0.0623 | 0.9910 |     0.0061 |     0.0059 |     0.0063 |     0.0059 |     0.0060 |    0.9906 |    0.9916 |    0.9905 |    0.9911 |    0.9910
    20.0 | 67.4 | 0.0620 | 0.9910 |     0.0059 |     0.0058 |     0.0061 |     0.0061 |     0.0059 |    0.9911 |    0.9909 |    0.9909 |    0.9910 |    0.9910
    30.0 | 67.6 | 0.0620 | 0.9910 |     0.0060 |     0.0058 |     0.0061 |     0.0061 |     0.0059 |    0.9910 |    0.9907 |    0.9909 |    0.9910 |    0.9912
    40.0 | 67.8 | 0.0616 | 0.9912 |     0.0059 |     0.0058 |     0.0059 |     0.0060 |     0.0058 |    0.9912 |    0.9910 |    0.9912 |    0.9912 |    0.9913
    50.0 | 68.0 | 0.0615 | 0.9911 |     0.0059 |     0.0058 |     0.0059 |     0.0060 |     0.0058 |    0.9912 |    0.9909 |    0.9911 |    0.9912 |    0.9913
    60.0 | 68.2 | 0.0614 | 0.9912 |     0.0059 |     0.0058 |     0.0059 |     0.0059 |     0.0058 |    0.9911 |    0.9910 |    0.9911 |    0.9913 |    0.9913
    70.0 | 68.4 | 0.0613 | 0.9912 |     0.0059 |     0.0058 |     0.0059 |     0.0059 |     0.0058 |    0.9911 |    0.9911 |    0.9912 |    0.9913 |    0.9914
    80.0 | 68.6 | 0.0612 | 0.9912 |     0.0059 |     0.0058 |     0.0058 |     0.0059 |     0.0058 |    0.9911 |    0.9911 |    0.9912 |    0.9913 |    0.9914
    90.1 | 68.8 | 0.0612 | 0.9912 |     0.0059 |     0.0058 |     0.0058 |     0.0059 |     0.0058 |    0.9911 |    0.9911 |    0.9912 |    0.9913 |    0.9913
   100.0 | 69.0 | 0.0611 | 0.9912 |     0.0058 |     0.0058 |     0.0058 |     0.0059 |     0.0058 |    0.9912 |    0.9911 |    0.9912 |    0.9913 |    0.9913
Epoch 00021: val_loss improved from 0.09020 to 0.08896, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0611 | 0.9912 |     0.0058 |     0.0058 |     0.0058 |     0.0058 |     0.0059 |    0.9911 |    0.9913 |    0.9913 |    0.9912 |    0.9912
   val | 0.0890 | 0.9909 |     0.0115 |     0.0113 |     0.0113 |     0.0114 |     0.0115 |    0.9909 |    0.9911 |    0.9910 |    0.9908 |    0.9909
Learning rate: 5.73e-05
====================================================================================================
Epoch 23/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 70.2 | 0.0501 | 0.9950 |     0.0028 |     0.0015 |     0.0105 |     0.0019 |     0.0014 |    1.0000 |    1.0000 |    0.9750 |    1.0000 |    1.0000
    10.0 | 70.4 | 0.0607 | 0.9911 |     0.0059 |     0.0056 |     0.0057 |     0.0058 |     0.0059 |    0.9910 |    0.9908 |    0.9911 |    0.9907 |    0.9917
    20.0 | 70.6 | 0.0595 | 0.9917 |     0.0056 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |    0.9916 |    0.9916 |    0.9917 |    0.9915 |    0.9919
    30.0 | 70.8 | 0.0597 | 0.9915 |     0.0056 |     0.0055 |     0.0055 |     0.0056 |     0.0055 |    0.9916 |    0.9915 |    0.9915 |    0.9916 |    0.9915
    40.0 | 71.0 | 0.0602 | 0.9913 |     0.0056 |     0.0057 |     0.0057 |     0.0057 |     0.0056 |    0.9915 |    0.9913 |    0.9912 |    0.9912 |    0.9913
    50.0 | 71.2 | 0.0602 | 0.9913 |     0.0057 |     0.0057 |     0.0057 |     0.0057 |     0.0057 |    0.9914 |    0.9914 |    0.9912 |    0.9912 |    0.9913
    60.0 | 71.4 | 0.0603 | 0.9913 |     0.0057 |     0.0057 |     0.0057 |     0.0057 |     0.0057 |    0.9913 |    0.9913 |    0.9912 |    0.9913 |    0.9914
    70.0 | 71.6 | 0.0602 | 0.9913 |     0.0057 |     0.0057 |     0.0057 |     0.0057 |     0.0056 |    0.9912 |    0.9913 |    0.9913 |    0.9913 |    0.9915
    80.0 | 71.8 | 0.0604 | 0.9912 |     0.0058 |     0.0057 |     0.0057 |     0.0058 |     0.0057 |    0.9911 |    0.9912 |    0.9912 |    0.9913 |    0.9914
    90.0 | 72.0 | 0.0605 | 0.9912 |     0.0058 |     0.0057 |     0.0058 |     0.0058 |     0.0057 |    0.9911 |    0.9911 |    0.9911 |    0.9912 |    0.9914
   100.0 | 72.2 | 0.0604 | 0.9912 |     0.0058 |     0.0057 |     0.0057 |     0.0058 |     0.0057 |    0.9912 |    0.9911 |    0.9912 |    0.9912 |    0.9914
Epoch 00022: val_loss improved from 0.08896 to 0.08798, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0604 | 0.9912 |     0.0058 |     0.0057 |     0.0057 |     0.0057 |     0.0058 |    0.9911 |    0.9914 |    0.9912 |    0.9912 |    0.9912
   val | 0.0880 | 0.9911 |     0.0114 |     0.0112 |     0.0112 |     0.0113 |     0.0114 |    0.9910 |    0.9912 |    0.9911 |    0.9910 |    0.9910
Learning rate: 5.59e-05
====================================================================================================
Epoch 24/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 73.4 | 0.0577 | 0.9952 |     0.0054 |     0.0128 |     0.0024 |     0.0029 |     0.0027 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    0.9762
    10.0 | 73.6 | 0.0602 | 0.9914 |     0.0059 |     0.0058 |     0.0057 |     0.0057 |     0.0055 |    0.9910 |    0.9915 |    0.9914 |    0.9916 |    0.9914
    20.0 | 73.8 | 0.0604 | 0.9913 |     0.0057 |     0.0060 |     0.0058 |     0.0058 |     0.0057 |    0.9913 |    0.9914 |    0.9914 |    0.9913 |    0.9909
    30.0 | 74.0 | 0.0606 | 0.9912 |     0.0059 |     0.0059 |     0.0058 |     0.0058 |     0.0057 |    0.9909 |    0.9914 |    0.9914 |    0.9914 |    0.9911
    40.0 | 74.2 | 0.0606 | 0.9912 |     0.0059 |     0.0058 |     0.0059 |     0.0058 |     0.0057 |    0.9909 |    0.9912 |    0.9912 |    0.9913 |    0.9912
    50.0 | 74.4 | 0.0603 | 0.9911 |     0.0058 |     0.0057 |     0.0058 |     0.0058 |     0.0057 |    0.9911 |    0.9910 |    0.9912 |    0.9912 |    0.9913INFO (2017-07-25 22:12:25,097): Done!

    60.0 | 74.6 | 0.0602 | 0.9912 |     0.0058 |     0.0057 |     0.0058 |     0.0058 |     0.0057 |    0.9910 |    0.9911 |    0.9912 |    0.9912 |    0.9913
    70.0 | 74.8 | 0.0601 | 0.9912 |     0.0058 |     0.0057 |     0.0058 |     0.0058 |     0.0057 |    0.9910 |    0.9912 |    0.9912 |    0.9913 |    0.9914
    80.0 | 75.0 | 0.0600 | 0.9912 |     0.0058 |     0.0057 |     0.0058 |     0.0058 |     0.0057 |    0.9910 |    0.9911 |    0.9912 |    0.9913 |    0.9914
    90.1 | 75.2 | 0.0600 | 0.9912 |     0.0058 |     0.0057 |     0.0058 |     0.0058 |     0.0057 |    0.9910 |    0.9912 |    0.9913 |    0.9912 |    0.9914
   100.0 | 75.4 | 0.0598 | 0.9913 |     0.0057 |     0.0057 |     0.0057 |     0.0058 |     0.0056 |    0.9912 |    0.9912 |    0.9914 |    0.9913 |    0.9914
Epoch 00023: val_loss improved from 0.08798 to 0.08763, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0598 | 0.9913 |     0.0057 |     0.0057 |     0.0056 |     0.0057 |     0.0058 |    0.9912 |    0.9914 |    0.9913 |    0.9912 |    0.9914
   val | 0.0876 | 0.9911 |     0.0113 |     0.0112 |     0.0111 |     0.0113 |     0.0113 |    0.9911 |    0.9912 |    0.9911 |    0.9909 |    0.9910
Learning rate: 5.45e-05
====================================================================================================
Epoch 25/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 76.6 | 0.0515 | 1.0000 |     0.0047 |     0.0047 |     0.0034 |     0.0046 |     0.0027 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 76.8 | 0.0591 | 0.9918 |     0.0057 |     0.0055 |     0.0055 |     0.0056 |     0.0054 |    0.9916 |    0.9916 |    0.9921 |    0.9916 |    0.9918
    20.0 | 77.0 | 0.0595 | 0.9915 |     0.0058 |     0.0056 |     0.0055 |     0.0057 |     0.0056 |    0.9913 |    0.9912 |    0.9918 |    0.9915 |    0.9917
    30.0 | 77.2 | 0.0592 | 0.9916 |     0.0057 |     0.0055 |     0.0055 |     0.0056 |     0.0056 |    0.9915 |    0.9914 |    0.9917 |    0.9916 |    0.9917
    40.0 | 77.4 | 0.0593 | 0.9914 |     0.0057 |     0.0056 |     0.0057 |     0.0056 |     0.0056 |    0.9913 |    0.9913 |    0.9914 |    0.9915 |    0.9916
    50.0 | 77.6 | 0.0594 | 0.9914 |     0.0057 |     0.0056 |     0.0056 |     0.0057 |     0.0056 |    0.9913 |    0.9913 |    0.9914 |    0.9915 |    0.9915
    60.0 | 77.8 | 0.0594 | 0.9913 |     0.0057 |     0.0057 |     0.0056 |     0.0057 |     0.0056 |    0.9913 |    0.9912 |    0.9913 |    0.9914 |    0.9915
    70.0 | 78.0 | 0.0592 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |     0.0056 |    0.9915 |    0.9912 |    0.9914 |    0.9914 |    0.9916
    80.0 | 78.2 | 0.0593 | 0.9913 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |     0.0056 |    0.9914 |    0.9912 |    0.9913 |    0.9914 |    0.9915
    90.1 | 78.4 | 0.0593 | 0.9914 |     0.0057 |     0.0056 |     0.0056 |     0.0057 |     0.0056 |    0.9913 |    0.9912 |    0.9914 |    0.9914 |    0.9915
   100.0 | 78.6 | 0.0593 | 0.9914 |     0.0057 |     0.0056 |     0.0057 |     0.0057 |     0.0056 |    0.9913 |    0.9912 |    0.9914 |    0.9914 |    0.9915
Epoch 00024: val_loss improved from 0.08763 to 0.08617, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0593 | 0.9914 |     0.0057 |     0.0056 |     0.0056 |     0.0057 |     0.0057 |    0.9912 |    0.9915 |    0.9914 |    0.9913 |    0.9914
   val | 0.0862 | 0.9913 |     0.0111 |     0.0110 |     0.0110 |     0.0111 |     0.0112 |    0.9912 |    0.9915 |    0.9914 |    0.9913 |    0.9913
Learning rate: 5.31e-05
====================================================================================================
Epoch 26/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 79.8 | 0.0533 | 0.9819 |     0.0007 |     0.0007 |     0.0068 |     0.0101 |     0.0041 |    1.0000 |    0.9592 |    0.9773 |    0.9730 |    1.0000
    10.0 | 80.0 | 0.0581 | 0.9917 |     0.0055 |     0.0056 |     0.0053 |     0.0056 |     0.0053 |    0.9920 |    0.9914 |    0.9919 |    0.9920 |    0.9914
    20.0 | 80.2 | 0.0588 | 0.9914 |     0.0056 |     0.0057 |     0.0055 |     0.0056 |     0.0055 |    0.9915 |    0.9914 |    0.9913 |    0.9915 |    0.9913
    30.0 | 80.4 | 0.0586 | 0.9915 |     0.0055 |     0.0057 |     0.0055 |     0.0055 |     0.0055 |    0.9916 |    0.9917 |    0.9914 |    0.9915 |    0.9913
    40.0 | 80.6 | 0.0585 | 0.9915 |     0.0055 |     0.0057 |     0.0055 |     0.0055 |     0.0055 |    0.9917 |    0.9915 |    0.9915 |    0.9915 |    0.9914
    50.0 | 80.8 | 0.0586 | 0.9916 |     0.0055 |     0.0056 |     0.0055 |     0.0056 |     0.0055 |    0.9917 |    0.9915 |    0.9915 |    0.9916 |    0.9915
    60.0 | 81.0 | 0.0587 | 0.9915 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9915 |    0.9915 |    0.9914 |    0.9916 |    0.9916
    70.0 | 81.2 | 0.0587 | 0.9915 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9915 |    0.9914 |    0.9913 |    0.9916 |    0.9916
    80.1 | 81.4 | 0.0588 | 0.9915 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9914 |    0.9914 |    0.9914 |    0.9916 |    0.9915
    90.1 | 81.6 | 0.0588 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9914 |    0.9914 |    0.9913 |    0.9915 |    0.9916
   100.0 | 81.8 | 0.0588 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |     0.0056 |    0.9914 |    0.9914 |    0.9914 |    0.9914 |    0.9915
Epoch 00025: val_loss did not improve

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0588 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |    0.9914 |    0.9915 |    0.9914 |    0.9914 |    0.9914
   val | 0.0865 | 0.9917 |     0.0112 |     0.0111 |     0.0111 |     0.0112 |     0.0113 |    0.9916 |    0.9919 |    0.9917 |    0.9917 |    0.9917
Learning rate: 5.18e-05
====================================================================================================
Epoch 27/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 83.0 | 0.0407 | 0.9951 |     0.0048 |     0.0013 |     0.0012 |     0.0012 |     0.0016 |    0.9756 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 83.2 | 0.0586 | 0.9914 |     0.0056 |     0.0055 |     0.0056 |     0.0056 |     0.0057 |    0.9914 |    0.9914 |    0.9914 |    0.9910 |    0.9920
    20.0 | 83.4 | 0.0588 | 0.9912 |     0.0058 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9910 |    0.9912 |    0.9911 |    0.9911 |    0.9914
    30.0 | 83.6 | 0.0586 | 0.9913 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |    0.9913 |    0.9913 |    0.9913 |    0.9911 |    0.9916
    40.0 | 83.8 | 0.0588 | 0.9912 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |     0.0057 |    0.9912 |    0.9912 |    0.9912 |    0.9910 |    0.9915
    50.0 | 84.0 | 0.0586 | 0.9913 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |     0.0056 |    0.9914 |    0.9913 |    0.9912 |    0.9911 |    0.9917
    60.0 | 84.2 | 0.0586 | 0.9913 |     0.0056 |     0.0056 |     0.0057 |     0.0057 |     0.0056 |    0.9913 |    0.9912 |    0.9912 |    0.9912 |    0.9917
    70.0 | 84.4 | 0.0585 | 0.9914 |     0.0056 |     0.0055 |     0.0056 |     0.0056 |     0.0056 |    0.9913 |    0.9913 |    0.9913 |    0.9913 |    0.9917
    80.0 | 84.6 | 0.0584 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9914 |    0.9914 |    0.9914 |    0.9914 |    0.9916
    90.0 | 84.8 | 0.0584 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0055 |    0.9914 |    0.9913 |    0.9914 |    0.9914 |    0.9916
   100.0 | 85.0 | 0.0583 | 0.9915 |     0.0056 |     0.0055 |     0.0056 |     0.0056 |     0.0055 |    0.9914 |    0.9914 |    0.9914 |    0.9915 |    0.9916
Epoch 00026: val_loss improved from 0.08617 to 0.08546, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0583 | 0.9915 |     0.0056 |     0.0055 |     0.0055 |     0.0056 |     0.0056 |    0.9914 |    0.9916 |    0.9915 |    0.9914 |    0.9914
   val | 0.0855 | 0.9916 |     0.0110 |     0.0109 |     0.0109 |     0.0110 |     0.0111 |    0.9915 |    0.9917 |    0.9916 |    0.9915 |    0.9916
Learning rate: 5.05e-05
====================================================================================================
Epoch 28/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 86.2 | 0.0434 | 1.0000 |     0.0017 |     0.0018 |     0.0019 |     0.0021 |     0.0052 |    1.0000 |    1.0000 |    1.0000 |    1.0000 |    1.0000
    10.0 | 86.4 | 0.0588 | 0.9914 |     0.0056 |     0.0054 |     0.0059 |     0.0057 |     0.0057 |    0.9912 |    0.9917 |    0.9908 |    0.9913 |    0.9918
    20.0 | 86.6 | 0.0585 | 0.9915 |     0.0055 |     0.0055 |     0.0058 |     0.0056 |     0.0057 |    0.9916 |    0.9918 |    0.9912 |    0.9913 |    0.9918
    30.0 | 86.8 | 0.0585 | 0.9915 |     0.0056 |     0.0055 |     0.0057 |     0.0057 |     0.0056 |    0.9914 |    0.9916 |    0.9913 |    0.9915 |    0.9916
    40.0 | 87.0 | 0.0585 | 0.9915 |     0.0056 |     0.0055 |     0.0057 |     0.0056 |     0.0056 |    0.9913 |    0.9916 |    0.9914 |    0.9914 |    0.9915
    50.0 | 87.2 | 0.0585 | 0.9914 |     0.0057 |     0.0055 |     0.0058 |     0.0057 |     0.0056 |    0.9913 |    0.9915 |    0.9914 |    0.9914 |    0.9916
    60.0 | 87.4 | 0.0585 | 0.9914 |     0.0057 |     0.0055 |     0.0057 |     0.0057 |     0.0056 |    0.9913 |    0.9915 |    0.9915 |    0.9913 |    0.9916
    70.0 | 87.6 | 0.0581 | 0.9916 |     0.0056 |     0.0055 |     0.0056 |     0.0056 |     0.0055 |    0.9914 |    0.9917 |    0.9917 |    0.9916 |    0.9917
    80.1 | 87.8 | 0.0581 | 0.9916 |     0.0056 |     0.0055 |     0.0055 |     0.0056 |     0.0055 |    0.9914 |    0.9916 |    0.9917 |    0.9916 |    0.9916
    90.1 | 88.0 | 0.0581 | 0.9916 |     0.0056 |     0.0055 |     0.0055 |     0.0056 |     0.0055 |    0.9915 |    0.9916 |    0.9917 |    0.9916 |    0.9916
   100.0 | 88.2 | 0.0579 | 0.9916 |     0.0055 |     0.0055 |     0.0055 |     0.0056 |     0.0055 |    0.9916 |    0.9916 |    0.9917 |    0.9917 |    0.9917
Epoch 00027: val_loss improved from 0.08546 to 0.08470, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0579 | 0.9916 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |     0.0056 |    0.9916 |    0.9917 |    0.9917 |    0.9916 |    0.9917
   val | 0.0847 | 0.9916 |     0.0109 |     0.0108 |     0.0108 |     0.0109 |     0.0110 |    0.9915 |    0.9917 |    0.9916 |    0.9916 |    0.9916
Learning rate: 4.92e-05
====================================================================================================
Epoch 29/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 89.4 | 0.0472 | 0.9952 |     0.0042 |     0.0012 |     0.0009 |     0.0079 |     0.0029 |    1.0000 |    0.9762 |    1.0000 |    1.0000 |    1.0000
    10.0 | 89.6 | 0.0577 | 0.9916 |     0.0057 |     0.0055 |     0.0054 |     0.0054 |     0.0055 |    0.9916 |    0.9916 |    0.9916 |    0.9912 |    0.9919
    20.0 | 89.8 | 0.0574 | 0.9916 |     0.0056 |     0.0054 |     0.0054 |     0.0054 |     0.0055 |    0.9916 |    0.9917 |    0.9917 |    0.9914 |    0.9916
    30.0 | 90.0 | 0.0579 | 0.9915 |     0.0058 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |    0.9911 |    0.9916 |    0.9915 |    0.9916 |    0.9916
    40.0 | 90.2 | 0.0579 | 0.9915 |     0.0057 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |    0.9913 |    0.9915 |    0.9915 |    0.9915 |    0.9917
    50.0 | 90.4 | 0.0579 | 0.9915 |     0.0057 |     0.0055 |     0.0055 |     0.0056 |     0.0055 |    0.9913 |    0.9916 |    0.9915 |    0.9916 |    0.9917
    60.1 | 90.6 | 0.0577 | 0.9916 |     0.0056 |     0.0054 |     0.0055 |     0.0055 |     0.0054 |    0.9914 |    0.9916 |    0.9916 |    0.9917 |    0.9919
    70.1 | 90.8 | 0.0577 | 0.9916 |     0.0056 |     0.0055 |     0.0055 |     0.0056 |     0.0055 |    0.9914 |    0.9915 |    0.9916 |    0.9917 |    0.9917
    80.1 | 91.0 | 0.0574 | 0.9916 |     0.0055 |     0.0054 |     0.0055 |     0.0055 |     0.0054 |    0.9916 |    0.9915 |    0.9916 |    0.9917 |    0.9918
    90.1 | 91.2 | 0.0576 | 0.9916 |     0.0055 |     0.0054 |     0.0055 |     0.0056 |     0.0054 |    0.9915 |    0.9915 |    0.9916 |    0.9917 |    0.9917
   100.0 | 91.4 | 0.0575 | 0.9916 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |    0.9915 |    0.9915 |    0.9916 |    0.9916 |    0.9917
Epoch 00028: val_loss improved from 0.08470 to 0.08399, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0575 | 0.9916 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |    0.9915 |    0.9917 |    0.9916 |    0.9915 |    0.9916
   val | 0.0840 | 0.9918 |     0.0108 |     0.0108 |     0.0107 |     0.0108 |     0.0109 |    0.9916 |    0.9919 |    0.9918 |    0.9917 |    0.9918
Learning rate: 4.8e-05
====================================================================================================
Epoch 30/30
====================================================================================================
done (%) | time |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/3_loss | cpg/1_loss | cpg/5_loss | cpg/2_acc | cpg/1_acc | cpg/3_acc | cpg/5_acc | cpg/4_acc
--------------------------------------------------------------------------------------------------------------------------------------------------------------
     0.0 | 92.6 | 0.0792 | 0.9833 |     0.0044 |     0.0182 |     0.0042 |     0.0112 |     0.0113 |    1.0000 |    0.9800 |    1.0000 |    0.9773 |    0.9592
    10.0 | 92.8 | 0.0577 | 0.9916 |     0.0055 |     0.0057 |     0.0057 |     0.0054 |     0.0054 |    0.9916 |    0.9917 |    0.9915 |    0.9919 |    0.9912
    20.0 | 93.0 | 0.0573 | 0.9917 |     0.0055 |     0.0054 |     0.0056 |     0.0055 |     0.0053 |    0.9917 |    0.9915 |    0.9916 |    0.9921 |    0.9918
    30.0 | 93.2 | 0.0569 | 0.9920 |     0.0054 |     0.0053 |     0.0055 |     0.0054 |     0.0052 |    0.9920 |    0.9917 |    0.9918 |    0.9922 |    0.9921
    40.0 | 93.4 | 0.0567 | 0.9920 |     0.0054 |     0.0053 |     0.0054 |     0.0054 |     0.0052 |    0.9919 |    0.9918 |    0.9919 |    0.9922 |    0.9922
    50.0 | 93.6 | 0.0568 | 0.9919 |     0.0054 |     0.0053 |     0.0055 |     0.0054 |     0.0053 |    0.9919 |    0.9918 |    0.9918 |    0.9921 |    0.9921
    60.0 | 93.8 | 0.0570 | 0.9918 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |     0.0053 |    0.9918 |    0.9917 |    0.9917 |    0.9920 |    0.9920
    70.0 | 94.0 | 0.0570 | 0.9918 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |     0.0053 |    0.9918 |    0.9917 |    0.9917 |    0.9920 |    0.9919
    80.0 | 94.2 | 0.0570 | 0.9918 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |     0.0054 |    0.9918 |    0.9917 |    0.9917 |    0.9919 |    0.9919
    90.1 | 94.4 | 0.0571 | 0.9918 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |     0.0054 |    0.9918 |    0.9917 |    0.9917 |    0.9918 |    0.9919
   100.0 | 94.6 | 0.0572 | 0.9917 |     0.0055 |     0.0054 |     0.0055 |     0.0055 |     0.0054 |    0.9917 |    0.9916 |    0.9917 |    0.9918 |    0.9919
Epoch 00029: val_loss improved from 0.08399 to 0.08357, saving model to ./models/dna/model_weights_val.h5

 split |   loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
-----------------------------------------------------------------------------------------------------------------------------------------------------
 train | 0.0572 | 0.9917 |     0.0055 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |    0.9916 |    0.9919 |    0.9918 |    0.9917 |    0.9917
   val | 0.0836 | 0.9914 |     0.0108 |     0.0107 |     0.0107 |     0.0108 |     0.0109 |    0.9912 |    0.9915 |    0.9915 |    0.9914 |    0.9914
====================================================================================================

Training set performance:
  loss |    acc | cpg/2_loss | cpg/4_loss | cpg/5_loss | cpg/3_loss | cpg/1_loss | cpg/1_acc | cpg/4_acc | cpg/5_acc | cpg/2_acc | cpg/3_acc
--------------------------------------------------------------------------------------------------------------------------------------------
0.3935 | 0.8531 |     0.0608 |     0.0609 |     0.0607 |     0.0609 |     0.0605 |    0.8536 |    0.8526 |    0.8530 |    0.8530 |    0.8530
0.2241 | 0.9404 |     0.0304 |     0.0303 |     0.0304 |     0.0304 |     0.0304 |    0.9405 |    0.9404 |    0.9404 |    0.9402 |    0.9404
0.1708 | 0.9616 |     0.0209 |     0.0207 |     0.0208 |     0.0209 |     0.0208 |    0.9617 |    0.9619 |    0.9616 |    0.9615 |    0.9614
0.1411 | 0.9715 |     0.0161 |     0.0160 |     0.0161 |     0.0161 |     0.0161 |    0.9714 |    0.9716 |    0.9716 |    0.9714 |    0.9715
0.1213 | 0.9778 |     0.0131 |     0.0131 |     0.0131 |     0.0132 |     0.0131 |    0.9778 |    0.9777 |    0.9779 |    0.9777 |    0.9777
0.1081 | 0.9816 |     0.0113 |     0.0112 |     0.0112 |     0.0113 |     0.0113 |    0.9815 |    0.9817 |    0.9817 |    0.9815 |    0.9817
0.0986 | 0.9840 |     0.0101 |     0.0100 |     0.0100 |     0.0101 |     0.0101 |    0.9840 |    0.9841 |    0.9840 |    0.9839 |    0.9839
0.0922 | 0.9853 |     0.0093 |     0.0093 |     0.0093 |     0.0094 |     0.0094 |    0.9853 |    0.9854 |    0.9854 |    0.9852 |    0.9853
0.0870 | 0.9862 |     0.0087 |     0.0087 |     0.0087 |     0.0088 |     0.0088 |    0.9863 |    0.9863 |    0.9862 |    0.9861 |    0.9861
0.0822 | 0.9873 |     0.0081 |     0.0081 |     0.0081 |     0.0082 |     0.0082 |    0.9873 |    0.9873 |    0.9874 |    0.9872 |    0.9872
0.0782 | 0.9881 |     0.0077 |     0.0076 |     0.0076 |     0.0077 |     0.0077 |    0.9880 |    0.9882 |    0.9882 |    0.9880 |    0.9881
0.0751 | 0.9887 |     0.0074 |     0.0073 |     0.0073 |     0.0074 |     0.0074 |    0.9886 |    0.9888 |    0.9888 |    0.9886 |    0.9888
0.0722 | 0.9893 |     0.0071 |     0.0070 |     0.0070 |     0.0071 |     0.0071 |    0.9892 |    0.9893 |    0.9893 |    0.9892 |    0.9894
0.0699 | 0.9896 |     0.0068 |     0.0067 |     0.0067 |     0.0068 |     0.0068 |    0.9895 |    0.9896 |    0.9897 |    0.9896 |    0.9898
0.0682 | 0.9899 |     0.0066 |     0.0066 |     0.0065 |     0.0066 |     0.0067 |    0.9898 |    0.9899 |    0.9899 |    0.9898 |    0.9899
0.0668 | 0.9902 |     0.0065 |     0.0064 |     0.0064 |     0.0065 |     0.0065 |    0.9901 |    0.9903 |    0.9902 |    0.9901 |    0.9903
0.0656 | 0.9904 |     0.0063 |     0.0063 |     0.0062 |     0.0063 |     0.0063 |    0.9903 |    0.9905 |    0.9904 |    0.9904 |    0.9905
0.0643 | 0.9907 |     0.0062 |     0.0061 |     0.0061 |     0.0062 |     0.0062 |    0.9906 |    0.9908 |    0.9907 |    0.9906 |    0.9908
0.0633 | 0.9909 |     0.0061 |     0.0060 |     0.0060 |     0.0060 |     0.0061 |    0.9908 |    0.9910 |    0.9909 |    0.9907 |    0.9909
0.0625 | 0.9909 |     0.0060 |     0.0059 |     0.0059 |     0.0060 |     0.0060 |    0.9909 |    0.9911 |    0.9910 |    0.9908 |    0.9910
0.0617 | 0.9911 |     0.0059 |     0.0058 |     0.0058 |     0.0059 |     0.0059 |    0.9910 |    0.9913 |    0.9911 |    0.9910 |    0.9912
0.0611 | 0.9912 |     0.0058 |     0.0058 |     0.0058 |     0.0058 |     0.0059 |    0.9911 |    0.9913 |    0.9913 |    0.9912 |    0.9912
0.0604 | 0.9912 |     0.0058 |     0.0057 |     0.0057 |     0.0057 |     0.0058 |    0.9911 |    0.9914 |    0.9912 |    0.9912 |    0.9912
0.0598 | 0.9913 |     0.0057 |     0.0057 |     0.0056 |     0.0057 |     0.0058 |    0.9912 |    0.9914 |    0.9913 |    0.9912 |    0.9914
0.0593 | 0.9914 |     0.0057 |     0.0056 |     0.0056 |     0.0057 |     0.0057 |    0.9912 |    0.9915 |    0.9914 |    0.9913 |    0.9914
0.0588 | 0.9914 |     0.0056 |     0.0056 |     0.0056 |     0.0056 |     0.0057 |    0.9914 |    0.9915 |    0.9914 |    0.9914 |    0.9914
0.0583 | 0.9915 |     0.0056 |     0.0055 |     0.0055 |     0.0056 |     0.0056 |    0.9914 |    0.9916 |    0.9915 |    0.9914 |    0.9914
0.0579 | 0.9916 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |     0.0056 |    0.9916 |    0.9917 |    0.9917 |    0.9916 |    0.9917
0.0575 | 0.9916 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |     0.0055 |    0.9915 |    0.9917 |    0.9916 |    0.9915 |    0.9916
0.0572 | 0.9917 |     0.0055 |     0.0054 |     0.0054 |     0.0055 |     0.0055 |    0.9916 |    0.9919 |    0.9918 |    0.9917 |    0.9917

Validation set performance:
  loss |    acc | cpg/3_loss | cpg/2_loss | cpg/1_loss | cpg/4_loss | cpg/5_loss | cpg/3_acc | cpg/4_acc | cpg/2_acc | cpg/5_acc | cpg/1_acc
--------------------------------------------------------------------------------------------------------------------------------------------
0.4601 | 0.9215 |     0.0776 |     0.0773 |     0.0772 |     0.0771 |     0.0771 |    0.9212 |    0.9214 |    0.9214 |    0.9221 |    0.9216
0.3084 | 0.9555 |     0.0478 |     0.0478 |     0.0477 |     0.0477 |     0.0477 |    0.9555 |    0.9555 |    0.9553 |    0.9557 |    0.9557
0.2421 | 0.9674 |     0.0357 |     0.0357 |     0.0357 |     0.0355 |     0.0357 |    0.9674 |    0.9676 |    0.9673 |    0.9674 |    0.9673
0.2140 | 0.9716 |     0.0313 |     0.0312 |     0.0314 |     0.0312 |     0.0311 |    0.9715 |    0.9716 |    0.9717 |    0.9718 |    0.9713
0.1710 | 0.9805 |     0.0236 |     0.0235 |     0.0235 |     0.0234 |     0.0234 |    0.9805 |    0.9805 |    0.9805 |    0.9807 |    0.9805
0.1537 | 0.9834 |     0.0209 |     0.0208 |     0.0208 |     0.0206 |     0.0206 |    0.9833 |    0.9835 |    0.9833 |    0.9836 |    0.9834
0.1470 | 0.9840 |     0.0201 |     0.0201 |     0.0200 |     0.0200 |     0.0199 |    0.9839 |    0.9840 |    0.9838 |    0.9841 |    0.9840
0.1530 | 0.9809 |     0.0218 |     0.0217 |     0.0215 |     0.0215 |     0.0215 |    0.9807 |    0.9810 |    0.9807 |    0.9811 |    0.9810
0.1248 | 0.9868 |     0.0166 |     0.0165 |     0.0166 |     0.0164 |     0.0164 |    0.9869 |    0.9869 |    0.9867 |    0.9869 |    0.9868
0.1195 | 0.9872 |     0.0159 |     0.0158 |     0.0158 |     0.0157 |     0.0157 |    0.9871 |    0.9873 |    0.9870 |    0.9873 |    0.9872
0.1135 | 0.9881 |     0.0149 |     0.0150 |     0.0150 |     0.0148 |     0.0148 |    0.9882 |    0.9882 |    0.9880 |    0.9882 |    0.9881
0.1086 | 0.9889 |     0.0143 |     0.0143 |     0.0143 |     0.0142 |     0.0141 |    0.9889 |    0.9888 |    0.9888 |    0.9889 |    0.9888
0.1060 | 0.9899 |     0.0140 |     0.0140 |     0.0141 |     0.0138 |     0.0138 |    0.9900 |    0.9900 |    0.9899 |    0.9899 |    0.9898
0.1023 | 0.9900 |     0.0134 |     0.0134 |     0.0135 |     0.0132 |     0.0132 |    0.9901 |    0.9901 |    0.9901 |    0.9900 |    0.9899
0.1003 | 0.9901 |     0.0130 |     0.0131 |     0.0131 |     0.0129 |     0.0129 |    0.9901 |    0.9902 |    0.9901 |    0.9901 |    0.9899
0.0976 | 0.9905 |     0.0127 |     0.0127 |     0.0128 |     0.0126 |     0.0125 |    0.9906 |    0.9906 |    0.9905 |    0.9905 |    0.9903
0.0955 | 0.9901 |     0.0124 |     0.0124 |     0.0124 |     0.0123 |     0.0122 |    0.9903 |    0.9901 |    0.9899 |    0.9902 |    0.9900
0.0936 | 0.9903 |     0.0121 |     0.0121 |     0.0122 |     0.0120 |     0.0120 |    0.9904 |    0.9904 |    0.9903 |    0.9904 |    0.9902
0.0923 | 0.9907 |     0.0119 |     0.0119 |     0.0120 |     0.0118 |     0.0118 |    0.9908 |    0.9908 |    0.9906 |    0.9907 |    0.9906
0.0917 | 0.9912 |     0.0119 |     0.0119 |     0.0120 |     0.0118 |     0.0117 |    0.9912 |    0.9913 |    0.9911 |    0.9912 |    0.9911
0.0902 | 0.9914 |     0.0116 |     0.0117 |     0.0117 |     0.0115 |     0.0115 |    0.9914 |    0.9915 |    0.9913 |    0.9913 |    0.9912
0.0890 | 0.9909 |     0.0114 |     0.0115 |     0.0115 |     0.0113 |     0.0113 |    0.9909 |    0.9911 |    0.9908 |    0.9910 |    0.9909
0.0880 | 0.9911 |     0.0113 |     0.0114 |     0.0114 |     0.0112 |     0.0112 |    0.9910 |    0.9912 |    0.9910 |    0.9911 |    0.9910
0.0876 | 0.9911 |     0.0113 |     0.0113 |     0.0113 |     0.0112 |     0.0111 |    0.9910 |    0.9912 |    0.9909 |    0.9911 |    0.9911
0.0862 | 0.9913 |     0.0111 |     0.0111 |     0.0112 |     0.0110 |     0.0110 |    0.9913 |    0.9915 |    0.9913 |    0.9914 |    0.9912
0.0865 | 0.9917 |     0.0112 |     0.0112 |     0.0113 |     0.0111 |     0.0111 |    0.9917 |    0.9919 |    0.9917 |    0.9917 |    0.9916
0.0855 | 0.9916 |     0.0110 |     0.0110 |     0.0111 |     0.0109 |     0.0109 |    0.9916 |    0.9917 |    0.9915 |    0.9916 |    0.9915
0.0847 | 0.9916 |     0.0109 |     0.0109 |     0.0110 |     0.0108 |     0.0108 |    0.9916 |    0.9917 |    0.9916 |    0.9916 |    0.9915
0.0840 | 0.9918 |     0.0108 |     0.0108 |     0.0109 |     0.0108 |     0.0107 |    0.9918 |    0.9919 |    0.9917 |    0.9918 |    0.9916
0.0836 | 0.9914 |     0.0108 |     0.0108 |     0.0109 |     0.0107 |     0.0107 |    0.9914 |    0.9915 |    0.9914 |    0.9915 |    0.9912
